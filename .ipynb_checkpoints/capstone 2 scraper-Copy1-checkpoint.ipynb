{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import get to call a get request on the site\n",
    "import requests\n",
    "\n",
    "#import to manipulate arrays with numpy\n",
    "import numpy as np\n",
    "\n",
    "#import to create, clean, and parse data frames with pandas\n",
    "import pandas as pd\n",
    "\n",
    "#import to enable datascraping\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#import to set up 'sleep' to wait between page loads\n",
    "import time\n",
    "\n",
    "#import to make that html readable\n",
    "import pprint\n",
    "\n",
    "#import regular expressions operations\n",
    "import re\n",
    "\n",
    "#import to get the universe in balance\n",
    "import random\n",
    "\n",
    "#import so we can do some heavy stats work\n",
    "import scipy as sp\n",
    "from scipy.stats import binom\n",
    "import scipy.stats as stats\n",
    "\n",
    "#import to access certain plotting features\n",
    "import seaborn as sns\n",
    "\n",
    "#import because we need its program functions\n",
    "import math\n",
    "\n",
    "#import because we need to plot and make it pretty\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will collect the links for all cities on craigslist. We will have to sparse through\n",
    "# these and cut out the non-US cities.\n",
    "def city_link_collector():\n",
    "    # this is the craigslist page with every city\n",
    "    main_page = requests.get('https://www.craigslist.org/about/sites')\n",
    "    soup = BeautifulSoup(main_page.text, 'html.parser')\n",
    "    \n",
    "    all_list = []\n",
    "    uscity_list = []\n",
    "    city_list = []\n",
    "    \n",
    "    for i in range(4):\n",
    "        for box in soup.find_all('div', class_='box box_{}'.format(i+1)):\n",
    "            all_list.append(box.find_all('a'))\n",
    "    for _ in all_list[:20:7]:\n",
    "        for __ in _:\n",
    "            uscity_list.append(__)\n",
    "    for ___ in all_list[20][0:94]:\n",
    "        uscity_list.append(___)\n",
    "    \n",
    "    for idx, city in enumerate(uscity_list):\n",
    "        city_list.append(str(uscity_list[idx]).split('''\"''')[1])\n",
    "    \n",
    "    return city_list\n",
    "    #                     #posting date\n",
    "    #                     #grab the datetime element 0 for date and 1 for time\n",
    "    #                     post_datetime = box.find('time', class_= 'result-date')['datetime']\n",
    "    #                     post_timing.append(post_datetime)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://auburn.craigslist.org/',\n",
       " 'https://bham.craigslist.org/',\n",
       " 'https://dothan.craigslist.org/',\n",
       " 'https://shoals.craigslist.org/',\n",
       " 'https://gadsden.craigslist.org/',\n",
       " 'https://huntsville.craigslist.org/',\n",
       " 'https://mobile.craigslist.org/',\n",
       " 'https://montgomery.craigslist.org/',\n",
       " 'https://tuscaloosa.craigslist.org/',\n",
       " 'https://anchorage.craigslist.org/',\n",
       " 'https://fairbanks.craigslist.org/',\n",
       " 'https://kenai.craigslist.org/',\n",
       " 'https://juneau.craigslist.org/',\n",
       " 'https://flagstaff.craigslist.org/',\n",
       " 'https://mohave.craigslist.org/',\n",
       " 'https://phoenix.craigslist.org/',\n",
       " 'https://prescott.craigslist.org/',\n",
       " 'https://showlow.craigslist.org/',\n",
       " 'https://sierravista.craigslist.org/',\n",
       " 'https://tucson.craigslist.org/',\n",
       " 'https://yuma.craigslist.org/',\n",
       " 'https://fayar.craigslist.org/',\n",
       " 'https://fortsmith.craigslist.org/',\n",
       " 'https://jonesboro.craigslist.org/',\n",
       " 'https://littlerock.craigslist.org/',\n",
       " 'https://texarkana.craigslist.org/',\n",
       " 'https://bakersfield.craigslist.org/',\n",
       " 'https://chico.craigslist.org/',\n",
       " 'https://fresno.craigslist.org/',\n",
       " 'https://goldcountry.craigslist.org/',\n",
       " 'https://hanford.craigslist.org/',\n",
       " 'https://humboldt.craigslist.org/',\n",
       " 'https://imperial.craigslist.org/',\n",
       " 'https://inlandempire.craigslist.org/',\n",
       " 'https://losangeles.craigslist.org/',\n",
       " 'https://mendocino.craigslist.org/',\n",
       " 'https://merced.craigslist.org/',\n",
       " 'https://modesto.craigslist.org/',\n",
       " 'https://monterey.craigslist.org/',\n",
       " 'https://orangecounty.craigslist.org/',\n",
       " 'https://palmsprings.craigslist.org/',\n",
       " 'https://redding.craigslist.org/',\n",
       " 'https://sacramento.craigslist.org/',\n",
       " 'https://sandiego.craigslist.org/',\n",
       " 'https://sfbay.craigslist.org/',\n",
       " 'https://slo.craigslist.org/',\n",
       " 'https://santabarbara.craigslist.org/',\n",
       " 'https://santamaria.craigslist.org/',\n",
       " 'https://siskiyou.craigslist.org/',\n",
       " 'https://stockton.craigslist.org/',\n",
       " 'https://susanville.craigslist.org/',\n",
       " 'https://ventura.craigslist.org/',\n",
       " 'https://visalia.craigslist.org/',\n",
       " 'https://yubasutter.craigslist.org/',\n",
       " 'https://boulder.craigslist.org/',\n",
       " 'https://cosprings.craigslist.org/',\n",
       " 'https://denver.craigslist.org/',\n",
       " 'https://eastco.craigslist.org/',\n",
       " 'https://fortcollins.craigslist.org/',\n",
       " 'https://rockies.craigslist.org/',\n",
       " 'https://pueblo.craigslist.org/',\n",
       " 'https://westslope.craigslist.org/',\n",
       " 'https://newlondon.craigslist.org/',\n",
       " 'https://hartford.craigslist.org/',\n",
       " 'https://newhaven.craigslist.org/',\n",
       " 'https://nwct.craigslist.org/',\n",
       " 'https://delaware.craigslist.org/',\n",
       " 'https://washingtondc.craigslist.org/',\n",
       " 'http://miami.craigslist.org/brw/',\n",
       " 'https://daytona.craigslist.org/',\n",
       " 'https://keys.craigslist.org/',\n",
       " 'https://fortlauderdale.craigslist.org/',\n",
       " 'https://fortmyers.craigslist.org/',\n",
       " 'https://gainesville.craigslist.org/',\n",
       " 'https://cfl.craigslist.org/',\n",
       " 'https://jacksonville.craigslist.org/',\n",
       " 'https://lakeland.craigslist.org/',\n",
       " 'http://miami.craigslist.org/mdc/',\n",
       " 'https://lakecity.craigslist.org/',\n",
       " 'https://ocala.craigslist.org/',\n",
       " 'https://okaloosa.craigslist.org/',\n",
       " 'https://orlando.craigslist.org/',\n",
       " 'https://panamacity.craigslist.org/',\n",
       " 'https://pensacola.craigslist.org/',\n",
       " 'https://sarasota.craigslist.org/',\n",
       " 'https://miami.craigslist.org/',\n",
       " 'https://spacecoast.craigslist.org/',\n",
       " 'https://staugustine.craigslist.org/',\n",
       " 'https://tallahassee.craigslist.org/',\n",
       " 'https://tampa.craigslist.org/',\n",
       " 'https://treasure.craigslist.org/',\n",
       " 'http://miami.craigslist.org/pbc/',\n",
       " 'https://albanyga.craigslist.org/',\n",
       " 'https://athensga.craigslist.org/',\n",
       " 'https://atlanta.craigslist.org/',\n",
       " 'https://augusta.craigslist.org/',\n",
       " 'https://brunswick.craigslist.org/',\n",
       " 'https://columbusga.craigslist.org/',\n",
       " 'https://macon.craigslist.org/',\n",
       " 'https://nwga.craigslist.org/',\n",
       " 'https://savannah.craigslist.org/',\n",
       " 'https://statesboro.craigslist.org/',\n",
       " 'https://valdosta.craigslist.org/',\n",
       " 'https://honolulu.craigslist.org/',\n",
       " 'https://boise.craigslist.org/',\n",
       " 'https://eastidaho.craigslist.org/',\n",
       " 'https://lewiston.craigslist.org/',\n",
       " 'https://twinfalls.craigslist.org/',\n",
       " 'https://bn.craigslist.org/',\n",
       " 'https://chambana.craigslist.org/',\n",
       " 'https://chicago.craigslist.org/',\n",
       " 'https://decatur.craigslist.org/',\n",
       " 'https://lasalle.craigslist.org/',\n",
       " 'https://mattoon.craigslist.org/',\n",
       " 'https://peoria.craigslist.org/',\n",
       " 'https://rockford.craigslist.org/',\n",
       " 'https://carbondale.craigslist.org/',\n",
       " 'https://springfieldil.craigslist.org/',\n",
       " 'https://quincy.craigslist.org/',\n",
       " 'https://bloomington.craigslist.org/',\n",
       " 'https://evansville.craigslist.org/',\n",
       " 'https://fortwayne.craigslist.org/',\n",
       " 'https://indianapolis.craigslist.org/',\n",
       " 'https://kokomo.craigslist.org/',\n",
       " 'https://tippecanoe.craigslist.org/',\n",
       " 'https://muncie.craigslist.org/',\n",
       " 'https://richmondin.craigslist.org/',\n",
       " 'https://southbend.craigslist.org/',\n",
       " 'https://terrehaute.craigslist.org/',\n",
       " 'https://ames.craigslist.org/',\n",
       " 'https://cedarrapids.craigslist.org/',\n",
       " 'https://desmoines.craigslist.org/',\n",
       " 'https://dubuque.craigslist.org/',\n",
       " 'https://fortdodge.craigslist.org/',\n",
       " 'https://iowacity.craigslist.org/',\n",
       " 'https://masoncity.craigslist.org/',\n",
       " 'https://quadcities.craigslist.org/',\n",
       " 'https://siouxcity.craigslist.org/',\n",
       " 'https://ottumwa.craigslist.org/',\n",
       " 'https://waterloo.craigslist.org/',\n",
       " 'https://lawrence.craigslist.org/',\n",
       " 'https://ksu.craigslist.org/',\n",
       " 'https://nwks.craigslist.org/',\n",
       " 'https://salina.craigslist.org/',\n",
       " 'https://seks.craigslist.org/',\n",
       " 'https://swks.craigslist.org/',\n",
       " 'https://topeka.craigslist.org/',\n",
       " 'https://wichita.craigslist.org/',\n",
       " 'https://bgky.craigslist.org/',\n",
       " 'https://eastky.craigslist.org/',\n",
       " 'https://lexington.craigslist.org/',\n",
       " 'https://louisville.craigslist.org/',\n",
       " 'https://owensboro.craigslist.org/',\n",
       " 'https://westky.craigslist.org/',\n",
       " 'https://batonrouge.craigslist.org/',\n",
       " 'https://cenla.craigslist.org/',\n",
       " 'https://houma.craigslist.org/',\n",
       " 'https://lafayette.craigslist.org/',\n",
       " 'https://lakecharles.craigslist.org/',\n",
       " 'https://monroe.craigslist.org/',\n",
       " 'https://neworleans.craigslist.org/',\n",
       " 'https://shreveport.craigslist.org/',\n",
       " 'https://maine.craigslist.org/',\n",
       " 'https://annapolis.craigslist.org/',\n",
       " 'https://baltimore.craigslist.org/',\n",
       " 'https://easternshore.craigslist.org/',\n",
       " 'https://frederick.craigslist.org/',\n",
       " 'https://smd.craigslist.org/',\n",
       " 'https://westmd.craigslist.org/',\n",
       " 'https://boston.craigslist.org/',\n",
       " 'https://capecod.craigslist.org/',\n",
       " 'https://southcoast.craigslist.org/',\n",
       " 'https://westernmass.craigslist.org/',\n",
       " 'https://worcester.craigslist.org/',\n",
       " 'https://annarbor.craigslist.org/',\n",
       " 'https://battlecreek.craigslist.org/',\n",
       " 'https://centralmich.craigslist.org/',\n",
       " 'https://detroit.craigslist.org/',\n",
       " 'https://flint.craigslist.org/',\n",
       " 'https://grandrapids.craigslist.org/',\n",
       " 'https://holland.craigslist.org/',\n",
       " 'https://jxn.craigslist.org/',\n",
       " 'https://kalamazoo.craigslist.org/',\n",
       " 'https://lansing.craigslist.org/',\n",
       " 'https://monroemi.craigslist.org/',\n",
       " 'https://muskegon.craigslist.org/',\n",
       " 'https://nmi.craigslist.org/',\n",
       " 'https://porthuron.craigslist.org/',\n",
       " 'https://saginaw.craigslist.org/',\n",
       " 'https://swmi.craigslist.org/',\n",
       " 'https://thumb.craigslist.org/',\n",
       " 'https://up.craigslist.org/',\n",
       " 'https://bemidji.craigslist.org/',\n",
       " 'https://brainerd.craigslist.org/',\n",
       " 'https://duluth.craigslist.org/',\n",
       " 'https://mankato.craigslist.org/',\n",
       " 'https://minneapolis.craigslist.org/',\n",
       " 'https://rmn.craigslist.org/',\n",
       " 'https://marshall.craigslist.org/',\n",
       " 'https://stcloud.craigslist.org/',\n",
       " 'https://gulfport.craigslist.org/',\n",
       " 'https://hattiesburg.craigslist.org/',\n",
       " 'https://jackson.craigslist.org/',\n",
       " 'https://meridian.craigslist.org/',\n",
       " 'https://northmiss.craigslist.org/',\n",
       " 'https://natchez.craigslist.org/',\n",
       " 'https://columbiamo.craigslist.org/',\n",
       " 'https://joplin.craigslist.org/',\n",
       " 'https://kansascity.craigslist.org/',\n",
       " 'https://kirksville.craigslist.org/',\n",
       " 'https://loz.craigslist.org/',\n",
       " 'https://semo.craigslist.org/',\n",
       " 'https://springfield.craigslist.org/',\n",
       " 'https://stjoseph.craigslist.org/',\n",
       " 'https://stlouis.craigslist.org/',\n",
       " 'https://billings.craigslist.org/',\n",
       " 'https://bozeman.craigslist.org/',\n",
       " 'https://butte.craigslist.org/',\n",
       " 'https://greatfalls.craigslist.org/',\n",
       " 'https://helena.craigslist.org/',\n",
       " 'https://kalispell.craigslist.org/',\n",
       " 'https://missoula.craigslist.org/',\n",
       " 'https://montana.craigslist.org/',\n",
       " 'https://grandisland.craigslist.org/',\n",
       " 'https://lincoln.craigslist.org/',\n",
       " 'https://northplatte.craigslist.org/',\n",
       " 'https://omaha.craigslist.org/',\n",
       " 'https://scottsbluff.craigslist.org/',\n",
       " 'https://elko.craigslist.org/',\n",
       " 'https://lasvegas.craigslist.org/',\n",
       " 'https://reno.craigslist.org/',\n",
       " 'https://nh.craigslist.org/',\n",
       " 'https://cnj.craigslist.org/',\n",
       " 'https://jerseyshore.craigslist.org/',\n",
       " 'https://newjersey.craigslist.org/',\n",
       " 'https://southjersey.craigslist.org/',\n",
       " 'https://albuquerque.craigslist.org/',\n",
       " 'https://clovis.craigslist.org/',\n",
       " 'https://farmington.craigslist.org/',\n",
       " 'https://lascruces.craigslist.org/',\n",
       " 'https://roswell.craigslist.org/',\n",
       " 'https://santafe.craigslist.org/',\n",
       " 'https://albany.craigslist.org/',\n",
       " 'https://binghamton.craigslist.org/',\n",
       " 'https://buffalo.craigslist.org/',\n",
       " 'https://catskills.craigslist.org/',\n",
       " 'https://chautauqua.craigslist.org/',\n",
       " 'https://elmira.craigslist.org/',\n",
       " 'https://fingerlakes.craigslist.org/',\n",
       " 'https://glensfalls.craigslist.org/',\n",
       " 'https://hudsonvalley.craigslist.org/',\n",
       " 'https://ithaca.craigslist.org/',\n",
       " 'https://longisland.craigslist.org/',\n",
       " 'https://newyork.craigslist.org/',\n",
       " 'https://oneonta.craigslist.org/',\n",
       " 'https://plattsburgh.craigslist.org/',\n",
       " 'https://potsdam.craigslist.org/',\n",
       " 'https://rochester.craigslist.org/',\n",
       " 'https://syracuse.craigslist.org/',\n",
       " 'https://twintiers.craigslist.org/',\n",
       " 'https://utica.craigslist.org/',\n",
       " 'https://watertown.craigslist.org/',\n",
       " 'https://asheville.craigslist.org/',\n",
       " 'https://boone.craigslist.org/',\n",
       " 'https://charlotte.craigslist.org/',\n",
       " 'https://eastnc.craigslist.org/',\n",
       " 'https://fayetteville.craigslist.org/',\n",
       " 'https://greensboro.craigslist.org/',\n",
       " 'https://hickory.craigslist.org/',\n",
       " 'https://onslow.craigslist.org/',\n",
       " 'https://outerbanks.craigslist.org/',\n",
       " 'https://raleigh.craigslist.org/',\n",
       " 'https://wilmington.craigslist.org/',\n",
       " 'https://winstonsalem.craigslist.org/',\n",
       " 'https://bismarck.craigslist.org/',\n",
       " 'https://fargo.craigslist.org/',\n",
       " 'https://grandforks.craigslist.org/',\n",
       " 'https://nd.craigslist.org/',\n",
       " 'https://akroncanton.craigslist.org/',\n",
       " 'https://ashtabula.craigslist.org/',\n",
       " 'https://athensohio.craigslist.org/',\n",
       " 'https://chillicothe.craigslist.org/',\n",
       " 'https://cincinnati.craigslist.org/',\n",
       " 'https://cleveland.craigslist.org/',\n",
       " 'https://columbus.craigslist.org/',\n",
       " 'https://dayton.craigslist.org/',\n",
       " 'https://limaohio.craigslist.org/',\n",
       " 'https://mansfield.craigslist.org/',\n",
       " 'https://sandusky.craigslist.org/',\n",
       " 'https://toledo.craigslist.org/',\n",
       " 'https://tuscarawas.craigslist.org/',\n",
       " 'https://youngstown.craigslist.org/',\n",
       " 'https://zanesville.craigslist.org/',\n",
       " 'https://lawton.craigslist.org/',\n",
       " 'https://enid.craigslist.org/',\n",
       " 'https://oklahomacity.craigslist.org/',\n",
       " 'https://stillwater.craigslist.org/',\n",
       " 'https://tulsa.craigslist.org/',\n",
       " 'https://bend.craigslist.org/',\n",
       " 'https://corvallis.craigslist.org/',\n",
       " 'https://eastoregon.craigslist.org/',\n",
       " 'https://eugene.craigslist.org/',\n",
       " 'https://klamath.craigslist.org/',\n",
       " 'https://medford.craigslist.org/',\n",
       " 'https://oregoncoast.craigslist.org/',\n",
       " 'https://portland.craigslist.org/',\n",
       " 'https://roseburg.craigslist.org/',\n",
       " 'https://salem.craigslist.org/',\n",
       " 'https://altoona.craigslist.org/',\n",
       " 'https://chambersburg.craigslist.org/',\n",
       " 'https://erie.craigslist.org/',\n",
       " 'https://harrisburg.craigslist.org/',\n",
       " 'https://lancaster.craigslist.org/',\n",
       " 'https://allentown.craigslist.org/',\n",
       " 'https://meadville.craigslist.org/',\n",
       " 'https://philadelphia.craigslist.org/',\n",
       " 'https://pittsburgh.craigslist.org/',\n",
       " 'https://poconos.craigslist.org/',\n",
       " 'https://reading.craigslist.org/',\n",
       " 'https://scranton.craigslist.org/',\n",
       " 'https://pennstate.craigslist.org/',\n",
       " 'https://williamsport.craigslist.org/',\n",
       " 'https://york.craigslist.org/',\n",
       " 'https://providence.craigslist.org/',\n",
       " 'https://charleston.craigslist.org/',\n",
       " 'https://columbia.craigslist.org/',\n",
       " 'https://florencesc.craigslist.org/',\n",
       " 'https://greenville.craigslist.org/',\n",
       " 'https://hiltonhead.craigslist.org/',\n",
       " 'https://myrtlebeach.craigslist.org/',\n",
       " 'https://nesd.craigslist.org/',\n",
       " 'https://csd.craigslist.org/',\n",
       " 'https://rapidcity.craigslist.org/',\n",
       " 'https://siouxfalls.craigslist.org/',\n",
       " 'https://sd.craigslist.org/',\n",
       " 'https://chattanooga.craigslist.org/',\n",
       " 'https://clarksville.craigslist.org/',\n",
       " 'https://cookeville.craigslist.org/',\n",
       " 'https://jacksontn.craigslist.org/',\n",
       " 'https://knoxville.craigslist.org/',\n",
       " 'https://memphis.craigslist.org/',\n",
       " 'https://nashville.craigslist.org/',\n",
       " 'https://tricities.craigslist.org/',\n",
       " 'https://abilene.craigslist.org/',\n",
       " 'https://amarillo.craigslist.org/',\n",
       " 'https://austin.craigslist.org/',\n",
       " 'https://beaumont.craigslist.org/',\n",
       " 'https://brownsville.craigslist.org/',\n",
       " 'https://collegestation.craigslist.org/',\n",
       " 'https://corpuschristi.craigslist.org/',\n",
       " 'https://dallas.craigslist.org/',\n",
       " 'https://nacogdoches.craigslist.org/',\n",
       " 'https://delrio.craigslist.org/',\n",
       " 'https://elpaso.craigslist.org/',\n",
       " 'https://galveston.craigslist.org/',\n",
       " 'https://houston.craigslist.org/',\n",
       " 'https://killeen.craigslist.org/',\n",
       " 'https://laredo.craigslist.org/',\n",
       " 'https://lubbock.craigslist.org/',\n",
       " 'https://mcallen.craigslist.org/',\n",
       " 'https://odessa.craigslist.org/',\n",
       " 'https://sanangelo.craigslist.org/',\n",
       " 'https://sanantonio.craigslist.org/',\n",
       " 'https://sanmarcos.craigslist.org/',\n",
       " 'https://bigbend.craigslist.org/',\n",
       " 'https://texoma.craigslist.org/',\n",
       " 'https://easttexas.craigslist.org/',\n",
       " 'https://victoriatx.craigslist.org/',\n",
       " 'https://waco.craigslist.org/',\n",
       " 'https://wichitafalls.craigslist.org/',\n",
       " 'https://logan.craigslist.org/',\n",
       " 'https://ogden.craigslist.org/',\n",
       " 'https://provo.craigslist.org/',\n",
       " 'https://saltlakecity.craigslist.org/',\n",
       " 'https://stgeorge.craigslist.org/',\n",
       " 'https://vermont.craigslist.org/',\n",
       " 'https://charlottesville.craigslist.org/',\n",
       " 'https://danville.craigslist.org/',\n",
       " 'https://fredericksburg.craigslist.org/',\n",
       " 'https://norfolk.craigslist.org/',\n",
       " 'https://harrisonburg.craigslist.org/',\n",
       " 'https://lynchburg.craigslist.org/',\n",
       " 'https://blacksburg.craigslist.org/',\n",
       " 'https://richmond.craigslist.org/',\n",
       " 'https://roanoke.craigslist.org/',\n",
       " 'https://swva.craigslist.org/',\n",
       " 'https://winchester.craigslist.org/',\n",
       " 'https://bellingham.craigslist.org/',\n",
       " 'https://kpr.craigslist.org/',\n",
       " 'https://moseslake.craigslist.org/',\n",
       " 'https://olympic.craigslist.org/',\n",
       " 'https://pullman.craigslist.org/',\n",
       " 'https://seattle.craigslist.org/',\n",
       " 'https://skagit.craigslist.org/',\n",
       " 'https://spokane.craigslist.org/',\n",
       " 'https://wenatchee.craigslist.org/',\n",
       " 'https://yakima.craigslist.org/',\n",
       " 'https://charlestonwv.craigslist.org/',\n",
       " 'https://martinsburg.craigslist.org/',\n",
       " 'https://huntington.craigslist.org/',\n",
       " 'https://morgantown.craigslist.org/',\n",
       " 'https://wheeling.craigslist.org/',\n",
       " 'https://parkersburg.craigslist.org/',\n",
       " 'https://swv.craigslist.org/',\n",
       " 'https://wv.craigslist.org/',\n",
       " 'https://appleton.craigslist.org/',\n",
       " 'https://eauclaire.craigslist.org/',\n",
       " 'https://greenbay.craigslist.org/',\n",
       " 'https://janesville.craigslist.org/',\n",
       " 'https://racine.craigslist.org/',\n",
       " 'https://lacrosse.craigslist.org/',\n",
       " 'https://madison.craigslist.org/',\n",
       " 'https://milwaukee.craigslist.org/',\n",
       " 'https://northernwi.craigslist.org/',\n",
       " 'https://sheboygan.craigslist.org/',\n",
       " 'https://wausau.craigslist.org/',\n",
       " 'https://wyoming.craigslist.org/']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_list = city_link_collector()\n",
    "city_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Https://Auburn.Craigslist.Org/ Pages = 1\n",
      "Https://Auburn.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "6\n",
      "\n",
      "\n",
      "Https://Auburn.Craigslist.Org/ complete!\n",
      "6 rows collected.\n",
      "Https://Bham.Craigslist.Org/ Pages = 1\n",
      "Https://Bham.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "99\n",
      "\n",
      "\n",
      "Https://Bham.Craigslist.Org/ complete!\n",
      "99 rows collected.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-07fc09300c27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#get the first page of the Austin motorcycle prices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}search/mca?s=0&bundleDuplicates=1'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m#parse through it and make it readable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mhtml_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             )\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mca_cert_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mca_cert_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mssl_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         )\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password)\u001b[0m\n\u001b[1;32m    368\u001b[0m     ) or IS_SECURETRANSPORT:\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mHAS_SNI\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mserver_hostname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         warnings.warn(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m                 \u001b[0mcnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWantReadError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/OpenSSL/SSL.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1931\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1932\u001b[0m         \"\"\"\n\u001b[0;32m-> 1933\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_do_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1934\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "post_timing = []\n",
    "post_hoods = []\n",
    "post_title_texts = []\n",
    "post_links = []\n",
    "post_prices = []\n",
    "city_names = []\n",
    "city_names = []\n",
    "\n",
    "for city in city_list[0:3]: \n",
    "    \n",
    "    #get the first page of the Austin motorcycle prices\n",
    "    response = requests.get('{}search/mca?s=0&bundleDuplicates=1'.format(city))\n",
    "    #parse through it and make it readable\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    #find the total number of posts to find the limit for each page\n",
    "    results_num = html_soup.find('div', class_= 'search-legend')\n",
    "    #pulled the total count of posts as the upper bound of the pages array\n",
    "    results_total = int(results_num.find('span', class_='totalcount').text) \n",
    "    #each page has 119 posts so each new page is defined as follows: s=120, s=240, s=360, and so on. So we need to step in size 120 in the np.arange function\n",
    "    pages = np.arange(0, results_total+1, 120)\n",
    "    print(\"{} Pages = {}\".format(city.title(), len(pages)))\n",
    "\n",
    "    iterations = 0\n",
    "\n",
    "    for page in pages:         \n",
    "        #get request\n",
    "        post_timing_ = []\n",
    "        post_hoods_ = []\n",
    "        post_title_texts_ = []\n",
    "        post_links_ = []\n",
    "        post_prices_ = []\n",
    "        city_names_ = []\n",
    "        \n",
    "        response = requests.get(\"{}search/mca?\".format(city) \n",
    "                       + \"s=\" #the parameter for defining the page number \n",
    "                       + str(page) #the page number in the pages array from earlier\n",
    "                       + \"&bundleDuplicates=1\")\n",
    "\n",
    "        time.sleep(random.randint(5,10))\n",
    "\n",
    "        #throw warning for status codes that are not 200\n",
    "        if response.status_code != 200:\n",
    "            warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "        #define the html text\n",
    "        page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        #define the posts\n",
    "        posts = page_html.find_all('li', class_= 'result-row')\n",
    "\n",
    "        #extract data by item\n",
    "        for post in posts:\n",
    "                                   \n",
    "            if post.find('span', class_ = 'result-hood') is not None:\n",
    "\n",
    "                #posting date\n",
    "                #grab the datetime element 0 for date and 1 for time\n",
    "                post_datetime = post.find('time', class_= 'result-date')['datetime']\n",
    "                post_timing_.append(post_datetime)\n",
    "\n",
    "                #neighborhoods\n",
    "                post_hood = post.find('span', class_= 'result-hood').text\n",
    "                post_hoods_.append(post_hood)\n",
    "\n",
    "                #title text\n",
    "                post_title = post.find('a', class_='result-title hdrlnk')\n",
    "                post_title_text = post_title.text\n",
    "                post_title_texts_.append(post_title_text)\n",
    "\n",
    "                #post link\n",
    "                post_link = post_title['href']\n",
    "                post_links_.append(post_link)\n",
    "\n",
    "                #removes the \\n whitespace from each side, removes the currency symbol, and turns it into an int\n",
    "                post_price = (post.a.text.strip().replace(\"$\", \"\")) \n",
    "                post_prices_.append(post_price)\n",
    "\n",
    "                #add the city's name to the row\n",
    "                city_names_.append(city)\n",
    "        iterations += 1\n",
    "        print(\"{} Page \".format(city.title()) + str(iterations) + \" of {} pages\".format(len(pages)) + \" scraped successfully!\")\n",
    "        print(len(post_links_))\n",
    "    post_timing.append(post_timing_)\n",
    "    post_hoods.append(post_hoods_)\n",
    "    post_title_texts.append(post_title_texts_)\n",
    "    post_links.append(post_links_)\n",
    "    post_prices.append(post_prices_)\n",
    "    city_names.append(city_names_)\n",
    "    \n",
    "        \n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"{} complete!\".format(city.title()))\n",
    "    print(str(len(post_links_)) + \" rows collected.\")\n",
    "len(post_timing), len(post_hoods), len(post_title_texts), len(post_links), len(post_prices), len(city_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " [['2020-05-07 18:10',\n",
       "   '2020-05-06 15:26',\n",
       "   '2020-05-04 23:00',\n",
       "   '2020-04-26 13:49',\n",
       "   '2020-04-26 13:13',\n",
       "   '2020-04-20 13:18'],\n",
       "  ['2020-05-09 21:42',\n",
       "   '2020-05-09 15:08',\n",
       "   '2020-05-09 13:53',\n",
       "   '2020-05-09 13:10',\n",
       "   '2020-05-09 10:10',\n",
       "   '2020-05-09 00:24',\n",
       "   '2020-05-08 06:51',\n",
       "   '2020-04-16 04:26',\n",
       "   '2020-05-07 20:21',\n",
       "   '2020-05-07 19:01',\n",
       "   '2020-05-07 18:59',\n",
       "   '2020-05-07 17:36',\n",
       "   '2020-05-07 14:32',\n",
       "   '2020-05-07 09:40',\n",
       "   '2020-05-06 07:36',\n",
       "   '2020-05-05 17:31',\n",
       "   '2020-05-05 14:03',\n",
       "   '2020-05-05 08:56',\n",
       "   '2020-05-05 08:40',\n",
       "   '2020-05-04 17:08',\n",
       "   '2020-05-04 16:00',\n",
       "   '2020-05-04 14:50',\n",
       "   '2020-05-04 14:31',\n",
       "   '2020-05-04 13:51',\n",
       "   '2020-05-04 09:08',\n",
       "   '2020-05-03 19:15',\n",
       "   '2020-05-03 16:55',\n",
       "   '2020-05-03 14:30',\n",
       "   '2020-05-03 13:50',\n",
       "   '2020-05-03 11:14',\n",
       "   '2020-05-02 20:31',\n",
       "   '2020-05-02 19:24',\n",
       "   '2020-05-02 09:36',\n",
       "   '2020-05-02 08:41',\n",
       "   '2020-05-02 06:53',\n",
       "   '2020-05-01 11:18',\n",
       "   '2020-04-30 20:27',\n",
       "   '2020-04-30 10:36',\n",
       "   '2020-04-30 10:24',\n",
       "   '2020-04-30 09:50',\n",
       "   '2020-04-30 09:34',\n",
       "   '2020-04-30 09:26',\n",
       "   '2020-04-30 09:10',\n",
       "   '2020-04-29 19:49',\n",
       "   '2020-04-29 19:48',\n",
       "   '2020-04-29 18:09',\n",
       "   '2020-04-29 10:59',\n",
       "   '2020-04-29 08:56',\n",
       "   '2020-04-28 11:12',\n",
       "   '2020-04-27 15:51',\n",
       "   '2020-04-27 15:40',\n",
       "   '2020-04-27 13:52',\n",
       "   '2020-04-27 13:43',\n",
       "   '2020-04-27 13:28',\n",
       "   '2020-04-27 11:04',\n",
       "   '2020-04-26 13:59',\n",
       "   '2020-04-25 19:44',\n",
       "   '2020-04-25 07:21',\n",
       "   '2020-04-25 05:19',\n",
       "   '2020-04-24 19:34',\n",
       "   '2020-04-24 15:30',\n",
       "   '2020-04-24 15:30',\n",
       "   '2020-04-24 15:10',\n",
       "   '2020-04-24 11:42',\n",
       "   '2020-04-24 07:12',\n",
       "   '2020-04-24 03:23',\n",
       "   '2020-04-23 21:14',\n",
       "   '2020-04-23 20:52',\n",
       "   '2020-04-23 18:54',\n",
       "   '2020-04-23 08:11',\n",
       "   '2020-04-22 22:02',\n",
       "   '2020-04-22 21:21',\n",
       "   '2020-04-21 21:05',\n",
       "   '2020-04-21 13:59',\n",
       "   '2020-04-21 13:57',\n",
       "   '2020-04-19 18:42',\n",
       "   '2020-04-18 23:28',\n",
       "   '2020-04-18 17:28',\n",
       "   '2020-04-18 17:22',\n",
       "   '2020-04-17 15:54',\n",
       "   '2020-04-17 12:27',\n",
       "   '2020-04-17 12:26',\n",
       "   '2020-04-17 11:07',\n",
       "   '2020-04-17 09:44',\n",
       "   '2020-04-16 20:44',\n",
       "   '2020-04-16 18:16',\n",
       "   '2020-04-16 13:44',\n",
       "   '2020-04-16 12:28',\n",
       "   '2020-04-16 09:06',\n",
       "   '2020-04-15 11:19',\n",
       "   '2020-04-14 16:02',\n",
       "   '2020-04-13 18:54',\n",
       "   '2020-04-13 11:55',\n",
       "   '2020-04-13 08:34',\n",
       "   '2020-04-12 12:32',\n",
       "   '2020-04-11 14:04',\n",
       "   '2020-04-10 15:41',\n",
       "   '2020-04-10 12:52',\n",
       "   '2020-04-10 12:32',\n",
       "   '2020-04-10 12:15'],\n",
       "  ['2020-05-09 09:06',\n",
       "   '2020-05-06 18:32',\n",
       "   '2020-05-06 13:27',\n",
       "   '2020-05-04 19:47',\n",
       "   '2020-05-03 06:33',\n",
       "   '2020-05-02 08:59',\n",
       "   '2020-05-01 12:33',\n",
       "   '2020-05-01 12:26',\n",
       "   '2020-05-01 12:20',\n",
       "   '2020-05-01 10:50',\n",
       "   '2020-05-01 10:47',\n",
       "   '2020-04-30 15:15',\n",
       "   '2020-04-29 14:17',\n",
       "   '2020-04-26 18:56',\n",
       "   '2020-04-25 19:12',\n",
       "   '2020-04-22 10:57',\n",
       "   '2020-04-19 15:38',\n",
       "   '2020-04-17 19:17',\n",
       "   '2020-04-17 19:11',\n",
       "   '2020-04-11 18:31']])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_timing), post_timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def craigslist_motorcycle_scraper(city_list):\n",
    "\n",
    "    title = []\n",
    "    price = []\n",
    "    neighborhood = []\n",
    "    attributes = []\n",
    "    description = []\n",
    "    city_names = []\n",
    "\n",
    "    for city in city_list: \n",
    "\n",
    "        #get the first page of the Austin motorcycle prices\n",
    "        response = requests.get('{}search/mca?s=0&bundleDuplicates=1'.format(city))\n",
    "        #parse through it and make it readable\n",
    "        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        #find the total number of posts to find the limit for each page\n",
    "        results_num = html_soup.find('div', class_= 'search-legend')\n",
    "        #pulled the total count of posts as the upper bound of the pages array\n",
    "        results_total = int(results_num.find('span', class_='totalcount').text) \n",
    "        #each page has 119 posts so each new page is defined as follows: s=120, s=240, s=360, and so on. So we need to step in size 120 in the np.arange function\n",
    "        pages = np.arange(0, results_total+1, 120)\n",
    "        print(\"{} Posts = {}\".format(city, results_total))\n",
    "        print(\"{} Pages = {}\".format(city.title(), len(pages)))\n",
    "\n",
    "        iterations = 0\n",
    "\n",
    "        for page in pages:         \n",
    "            title_ = []\n",
    "            price_ = []\n",
    "            neighborhood_ = []\n",
    "            attributes_ = []\n",
    "            description_ = []\n",
    "            city_names_ = []\n",
    "            #get request      \n",
    "            response = requests.get(\"{}search/mca?\".format(city) \n",
    "                           + \"s=\" #the parameter for defining the page number \n",
    "                           + str(page) #the page number in the pages array from earlier\n",
    "                           + \"&bundleDuplicates=1\")\n",
    "\n",
    "            time.sleep(random.randint(2,3))\n",
    "\n",
    "            #throw warning for status codes that are not 200\n",
    "            if response.status_code != 200:\n",
    "                warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "            #define the html text\n",
    "            post_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            #define the posts\n",
    "    #         posts = page_html.find_all('li', class_= 'result-row')\n",
    "\n",
    "            #extract data by item\n",
    "    #         for post in posts:\n",
    "            count = 0\n",
    "            for post in post_soup.find_all('a', class_ = 'result-title hdrlnk'):\n",
    "                link = post['href']\n",
    "                sub_post = requests.get(link)\n",
    "                sub_soup = BeautifulSoup(sub_post.text, 'html.parser')\n",
    "\n",
    "    #             if tag.find('small') is not None:\n",
    "\n",
    "                post_title = sub_soup.find('span', id = 'titletextonly')\n",
    "                title_.append(post_title)\n",
    "\n",
    "                post_price = sub_soup.find('span', class_ = 'price')\n",
    "                price_.append(post_price)\n",
    "\n",
    "                post_neighborhood = sub_soup.find('small')\n",
    "                neighborhood_.append(post_neighborhood)\n",
    "\n",
    "                post_attributes = sub_soup.find_all('p', attrs = {'class': 'attrgroup'})\n",
    "                attributes_.append(post_attributes)\n",
    "\n",
    "                post_description = sub_soup.find('section', id = 'postingbody')\n",
    "                description_.append(post_description)\n",
    "\n",
    "                city_names_.append(city)\n",
    "\n",
    "                time.sleep(random.randint(2,3))\n",
    "                count += 1\n",
    "                \n",
    "                if count == results_total:\n",
    "                    break\n",
    "\n",
    "            iterations += 1\n",
    "            print(\"{} Page \".format(city.title()) + str(iterations) + \" of {} pages\".format(len(pages)) + \" scraped successfully!\")\n",
    "\n",
    "            title.append(title_)\n",
    "            price.append(price_)\n",
    "            neighborhood.append(neighborhood_)\n",
    "            attributes.append(attributes_)\n",
    "            description.append(description_)\n",
    "            city_names.append(city_names_)\n",
    "\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"{} complete!\".format(city.title()))\n",
    "        print(str(len(title_)) + \" rows collected.\")\n",
    "        print(\"\\n\")\n",
    "    return title, price, neighborhood, attributes, description, city_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://phoenix.craigslist.org/'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_list[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://auburn.craigslist.org/ Posts = 7\n",
      "Https://Auburn.Craigslist.Org/ Pages = 1\n",
      "Https://Auburn.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Auburn.Craigslist.Org/ complete!\n",
      "7 rows collected.\n",
      "\n",
      "\n",
      "https://bham.craigslist.org/ Posts = 108\n",
      "Https://Bham.Craigslist.Org/ Pages = 1\n",
      "Https://Bham.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Bham.Craigslist.Org/ complete!\n",
      "108 rows collected.\n",
      "\n",
      "\n",
      "https://dothan.craigslist.org/ Posts = 22\n",
      "Https://Dothan.Craigslist.Org/ Pages = 1\n",
      "Https://Dothan.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Dothan.Craigslist.Org/ complete!\n",
      "22 rows collected.\n",
      "\n",
      "\n",
      "https://shoals.craigslist.org/ Posts = 9\n",
      "Https://Shoals.Craigslist.Org/ Pages = 1\n",
      "Https://Shoals.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Shoals.Craigslist.Org/ complete!\n",
      "9 rows collected.\n",
      "\n",
      "\n",
      "https://gadsden.craigslist.org/ Posts = 10\n",
      "Https://Gadsden.Craigslist.Org/ Pages = 1\n",
      "Https://Gadsden.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Gadsden.Craigslist.Org/ complete!\n",
      "10 rows collected.\n",
      "\n",
      "\n",
      "https://huntsville.craigslist.org/ Posts = 150\n",
      "Https://Huntsville.Craigslist.Org/ Pages = 2\n",
      "Https://Huntsville.Craigslist.Org/ Page 1 of 2 pages scraped successfully!\n",
      "Https://Huntsville.Craigslist.Org/ Page 2 of 2 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Huntsville.Craigslist.Org/ complete!\n",
      "30 rows collected.\n",
      "\n",
      "\n",
      "https://mobile.craigslist.org/ Posts = 37\n",
      "Https://Mobile.Craigslist.Org/ Pages = 1\n",
      "Https://Mobile.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Mobile.Craigslist.Org/ complete!\n",
      "37 rows collected.\n",
      "\n",
      "\n",
      "https://montgomery.craigslist.org/ Posts = 15\n",
      "Https://Montgomery.Craigslist.Org/ Pages = 1\n",
      "Https://Montgomery.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Montgomery.Craigslist.Org/ complete!\n",
      "15 rows collected.\n",
      "\n",
      "\n",
      "https://tuscaloosa.craigslist.org/ Posts = 6\n",
      "Https://Tuscaloosa.Craigslist.Org/ Pages = 1\n",
      "Https://Tuscaloosa.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Tuscaloosa.Craigslist.Org/ complete!\n",
      "6 rows collected.\n",
      "\n",
      "\n",
      "https://anchorage.craigslist.org/ Posts = 163\n",
      "Https://Anchorage.Craigslist.Org/ Pages = 2\n",
      "Https://Anchorage.Craigslist.Org/ Page 1 of 2 pages scraped successfully!\n",
      "Https://Anchorage.Craigslist.Org/ Page 2 of 2 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Anchorage.Craigslist.Org/ complete!\n",
      "43 rows collected.\n",
      "\n",
      "\n",
      "https://fairbanks.craigslist.org/ Posts = 30\n",
      "Https://Fairbanks.Craigslist.Org/ Pages = 1\n",
      "Https://Fairbanks.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Fairbanks.Craigslist.Org/ complete!\n",
      "30 rows collected.\n",
      "\n",
      "\n",
      "https://kenai.craigslist.org/ Posts = 15\n",
      "Https://Kenai.Craigslist.Org/ Pages = 1\n",
      "Https://Kenai.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Kenai.Craigslist.Org/ complete!\n",
      "15 rows collected.\n",
      "\n",
      "\n",
      "https://juneau.craigslist.org/ Posts = 5\n",
      "Https://Juneau.Craigslist.Org/ Pages = 1\n",
      "Https://Juneau.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Juneau.Craigslist.Org/ complete!\n",
      "5 rows collected.\n",
      "\n",
      "\n",
      "https://flagstaff.craigslist.org/ Posts = 55\n",
      "Https://Flagstaff.Craigslist.Org/ Pages = 1\n",
      "Https://Flagstaff.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Flagstaff.Craigslist.Org/ complete!\n",
      "55 rows collected.\n",
      "\n",
      "\n",
      "https://mohave.craigslist.org/ Posts = 111\n",
      "Https://Mohave.Craigslist.Org/ Pages = 1\n",
      "Https://Mohave.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Mohave.Craigslist.Org/ complete!\n",
      "111 rows collected.\n",
      "\n",
      "\n",
      "CPU times: user 1min 39s, sys: 1.41 s, total: 1min 41s\n",
      "Wall time: 38min 39s\n"
     ]
    }
   ],
   "source": [
    "%time titleto15, priceto15, neighborhoodto15, attributesto15, descriptionto15, cityto15 = craigslist_motorcycle_scraper(city_list[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://prescott.craigslist.org/ Posts = 45\n",
      "Https://Prescott.Craigslist.Org/ Pages = 1\n",
      "Https://Prescott.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Prescott.Craigslist.Org/ complete!\n",
      "45 rows collected.\n",
      "\n",
      "\n",
      "https://showlow.craigslist.org/ Posts = 15\n",
      "Https://Showlow.Craigslist.Org/ Pages = 1\n",
      "Https://Showlow.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Showlow.Craigslist.Org/ complete!\n",
      "15 rows collected.\n",
      "\n",
      "\n",
      "https://sierravista.craigslist.org/ Posts = 8\n",
      "Https://Sierravista.Craigslist.Org/ Pages = 1\n",
      "Https://Sierravista.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Sierravista.Craigslist.Org/ complete!\n",
      "8 rows collected.\n",
      "\n",
      "\n",
      "https://tucson.craigslist.org/ Posts = 519\n",
      "Https://Tucson.Craigslist.Org/ Pages = 5\n",
      "Https://Tucson.Craigslist.Org/ Page 1 of 5 pages scraped successfully!\n",
      "Https://Tucson.Craigslist.Org/ Page 2 of 5 pages scraped successfully!\n",
      "Https://Tucson.Craigslist.Org/ Page 3 of 5 pages scraped successfully!\n",
      "Https://Tucson.Craigslist.Org/ Page 4 of 5 pages scraped successfully!\n",
      "Https://Tucson.Craigslist.Org/ Page 5 of 5 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Tucson.Craigslist.Org/ complete!\n",
      "39 rows collected.\n",
      "\n",
      "\n",
      "https://yuma.craigslist.org/ Posts = 54\n",
      "Https://Yuma.Craigslist.Org/ Pages = 1\n",
      "Https://Yuma.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Yuma.Craigslist.Org/ complete!\n",
      "54 rows collected.\n",
      "\n",
      "\n",
      "https://fayar.craigslist.org/ Posts = 128\n",
      "Https://Fayar.Craigslist.Org/ Pages = 2\n",
      "Https://Fayar.Craigslist.Org/ Page 1 of 2 pages scraped successfully!\n",
      "Https://Fayar.Craigslist.Org/ Page 2 of 2 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Fayar.Craigslist.Org/ complete!\n",
      "8 rows collected.\n",
      "\n",
      "\n",
      "https://fortsmith.craigslist.org/ Posts = 74\n",
      "Https://Fortsmith.Craigslist.Org/ Pages = 1\n",
      "Https://Fortsmith.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Fortsmith.Craigslist.Org/ complete!\n",
      "74 rows collected.\n",
      "\n",
      "\n",
      "https://jonesboro.craigslist.org/ Posts = 20\n",
      "Https://Jonesboro.Craigslist.Org/ Pages = 1\n",
      "Https://Jonesboro.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Jonesboro.Craigslist.Org/ complete!\n",
      "20 rows collected.\n",
      "\n",
      "\n",
      "https://littlerock.craigslist.org/ Posts = 112\n",
      "Https://Littlerock.Craigslist.Org/ Pages = 1\n",
      "Https://Littlerock.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Littlerock.Craigslist.Org/ complete!\n",
      "112 rows collected.\n",
      "\n",
      "\n",
      "https://texarkana.craigslist.org/ Posts = 14\n",
      "Https://Texarkana.Craigslist.Org/ Pages = 1\n",
      "Https://Texarkana.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Texarkana.Craigslist.Org/ complete!\n",
      "14 rows collected.\n",
      "\n",
      "\n",
      "https://bakersfield.craigslist.org/ Posts = 150\n",
      "Https://Bakersfield.Craigslist.Org/ Pages = 2\n",
      "Https://Bakersfield.Craigslist.Org/ Page 1 of 2 pages scraped successfully!\n",
      "Https://Bakersfield.Craigslist.Org/ Page 2 of 2 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Bakersfield.Craigslist.Org/ complete!\n",
      "30 rows collected.\n",
      "\n",
      "\n",
      "https://chico.craigslist.org/ Posts = 51\n",
      "Https://Chico.Craigslist.Org/ Pages = 1\n",
      "Https://Chico.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Chico.Craigslist.Org/ complete!\n",
      "51 rows collected.\n",
      "\n",
      "\n",
      "https://fresno.craigslist.org/ Posts = 144\n",
      "Https://Fresno.Craigslist.Org/ Pages = 2\n",
      "Https://Fresno.Craigslist.Org/ Page 1 of 2 pages scraped successfully!\n",
      "Https://Fresno.Craigslist.Org/ Page 2 of 2 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Fresno.Craigslist.Org/ complete!\n",
      "24 rows collected.\n",
      "\n",
      "\n",
      "https://goldcountry.craigslist.org/ Posts = 47\n",
      "Https://Goldcountry.Craigslist.Org/ Pages = 1\n",
      "Https://Goldcountry.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Goldcountry.Craigslist.Org/ complete!\n",
      "47 rows collected.\n",
      "\n",
      "\n",
      "https://hanford.craigslist.org/ Posts = 5\n",
      "Https://Hanford.Craigslist.Org/ Pages = 1\n",
      "Https://Hanford.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Hanford.Craigslist.Org/ complete!\n",
      "5 rows collected.\n",
      "\n",
      "\n",
      "CPU times: user 2min 55s, sys: 3.07 s, total: 2min 58s\n",
      "Wall time: 1h 16min 14s\n"
     ]
    }
   ],
   "source": [
    "%time title16to30, price16to30, neighborhood16to30, attributes16to30, description16to30, city16to30 = craigslist_motorcycle_scraper(city_list[16:31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://humboldt.craigslist.org/ Posts = 65\n",
      "Https://Humboldt.Craigslist.Org/ Pages = 1\n",
      "Https://Humboldt.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Humboldt.Craigslist.Org/ complete!\n",
      "65 rows collected.\n",
      "\n",
      "\n",
      "https://imperial.craigslist.org/ Posts = 8\n",
      "Https://Imperial.Craigslist.Org/ Pages = 1\n",
      "Https://Imperial.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Imperial.Craigslist.Org/ complete!\n",
      "8 rows collected.\n",
      "\n",
      "\n",
      "https://inlandempire.craigslist.org/ Posts = 847\n",
      "Https://Inlandempire.Craigslist.Org/ Pages = 8\n",
      "Https://Inlandempire.Craigslist.Org/ Page 1 of 8 pages scraped successfully!\n",
      "Https://Inlandempire.Craigslist.Org/ Page 2 of 8 pages scraped successfully!\n",
      "Https://Inlandempire.Craigslist.Org/ Page 3 of 8 pages scraped successfully!\n",
      "Https://Inlandempire.Craigslist.Org/ Page 4 of 8 pages scraped successfully!\n",
      "Https://Inlandempire.Craigslist.Org/ Page 5 of 8 pages scraped successfully!\n",
      "Https://Inlandempire.Craigslist.Org/ Page 6 of 8 pages scraped successfully!\n",
      "Https://Inlandempire.Craigslist.Org/ Page 7 of 8 pages scraped successfully!\n",
      "Https://Inlandempire.Craigslist.Org/ Page 8 of 8 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Inlandempire.Craigslist.Org/ complete!\n",
      "7 rows collected.\n",
      "\n",
      "\n",
      "https://losangeles.craigslist.org/ Posts = 1492\n",
      "Https://Losangeles.Craigslist.Org/ Pages = 13\n",
      "Https://Losangeles.Craigslist.Org/ Page 1 of 13 pages scraped successfully!\n",
      "Https://Losangeles.Craigslist.Org/ Page 2 of 13 pages scraped successfully!\n",
      "Https://Losangeles.Craigslist.Org/ Page 3 of 13 pages scraped successfully!\n",
      "Https://Losangeles.Craigslist.Org/ Page 4 of 13 pages scraped successfully!\n",
      "Https://Losangeles.Craigslist.Org/ Page 5 of 13 pages scraped successfully!\n",
      "Https://Losangeles.Craigslist.Org/ Page 6 of 13 pages scraped successfully!\n",
      "Https://Losangeles.Craigslist.Org/ Page 7 of 13 pages scraped successfully!\n",
      "Https://Losangeles.Craigslist.Org/ Page 8 of 13 pages scraped successfully!\n",
      "Https://Losangeles.Craigslist.Org/ Page 9 of 13 pages scraped successfully!\n",
      "Https://Losangeles.Craigslist.Org/ Page 10 of 13 pages scraped successfully!\n",
      "Https://Losangeles.Craigslist.Org/ Page 11 of 13 pages scraped successfully!\n",
      "Https://Losangeles.Craigslist.Org/ Page 12 of 13 pages scraped successfully!\n",
      "Https://Losangeles.Craigslist.Org/ Page 13 of 13 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Losangeles.Craigslist.Org/ complete!\n",
      "49 rows collected.\n",
      "\n",
      "\n",
      "https://mendocino.craigslist.org/ Posts = 13\n",
      "Https://Mendocino.Craigslist.Org/ Pages = 1\n",
      "Https://Mendocino.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Mendocino.Craigslist.Org/ complete!\n",
      "13 rows collected.\n",
      "\n",
      "\n",
      "https://merced.craigslist.org/ Posts = 56\n",
      "Https://Merced.Craigslist.Org/ Pages = 1\n",
      "Https://Merced.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Merced.Craigslist.Org/ complete!\n",
      "56 rows collected.\n",
      "\n",
      "\n",
      "https://modesto.craigslist.org/ Posts = 137\n",
      "Https://Modesto.Craigslist.Org/ Pages = 2\n",
      "Https://Modesto.Craigslist.Org/ Page 1 of 2 pages scraped successfully!\n",
      "Https://Modesto.Craigslist.Org/ Page 2 of 2 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Modesto.Craigslist.Org/ complete!\n",
      "17 rows collected.\n",
      "\n",
      "\n",
      "https://monterey.craigslist.org/ Posts = 67\n",
      "Https://Monterey.Craigslist.Org/ Pages = 1\n",
      "Https://Monterey.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Monterey.Craigslist.Org/ complete!\n",
      "67 rows collected.\n",
      "\n",
      "\n",
      "https://orangecounty.craigslist.org/ Posts = 961\n",
      "Https://Orangecounty.Craigslist.Org/ Pages = 9\n",
      "Https://Orangecounty.Craigslist.Org/ Page 1 of 9 pages scraped successfully!\n",
      "Https://Orangecounty.Craigslist.Org/ Page 2 of 9 pages scraped successfully!\n",
      "Https://Orangecounty.Craigslist.Org/ Page 3 of 9 pages scraped successfully!\n",
      "Https://Orangecounty.Craigslist.Org/ Page 4 of 9 pages scraped successfully!\n",
      "Https://Orangecounty.Craigslist.Org/ Page 5 of 9 pages scraped successfully!\n",
      "Https://Orangecounty.Craigslist.Org/ Page 6 of 9 pages scraped successfully!\n",
      "Https://Orangecounty.Craigslist.Org/ Page 7 of 9 pages scraped successfully!\n",
      "Https://Orangecounty.Craigslist.Org/ Page 8 of 9 pages scraped successfully!\n",
      "Https://Orangecounty.Craigslist.Org/ Page 9 of 9 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Orangecounty.Craigslist.Org/ complete!\n",
      "0 rows collected.\n",
      "\n",
      "\n",
      "https://palmsprings.craigslist.org/ Posts = 49\n",
      "Https://Palmsprings.Craigslist.Org/ Pages = 1\n",
      "Https://Palmsprings.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Palmsprings.Craigslist.Org/ complete!\n",
      "49 rows collected.\n",
      "\n",
      "\n",
      "https://redding.craigslist.org/ Posts = 72\n",
      "Https://Redding.Craigslist.Org/ Pages = 1\n",
      "Https://Redding.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Redding.Craigslist.Org/ complete!\n",
      "72 rows collected.\n",
      "\n",
      "\n",
      "https://sacramento.craigslist.org/ Posts = 1148\n",
      "Https://Sacramento.Craigslist.Org/ Pages = 10\n",
      "Https://Sacramento.Craigslist.Org/ Page 1 of 10 pages scraped successfully!\n",
      "Https://Sacramento.Craigslist.Org/ Page 2 of 10 pages scraped successfully!\n",
      "Https://Sacramento.Craigslist.Org/ Page 3 of 10 pages scraped successfully!\n",
      "Https://Sacramento.Craigslist.Org/ Page 4 of 10 pages scraped successfully!\n",
      "Https://Sacramento.Craigslist.Org/ Page 5 of 10 pages scraped successfully!\n",
      "Https://Sacramento.Craigslist.Org/ Page 6 of 10 pages scraped successfully!\n",
      "Https://Sacramento.Craigslist.Org/ Page 7 of 10 pages scraped successfully!\n",
      "Https://Sacramento.Craigslist.Org/ Page 8 of 10 pages scraped successfully!\n",
      "Https://Sacramento.Craigslist.Org/ Page 9 of 10 pages scraped successfully!\n",
      "Https://Sacramento.Craigslist.Org/ Page 10 of 10 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Sacramento.Craigslist.Org/ complete!\n",
      "67 rows collected.\n",
      "\n",
      "\n",
      "https://sandiego.craigslist.org/ Posts = 1389\n",
      "Https://Sandiego.Craigslist.Org/ Pages = 12\n",
      "Https://Sandiego.Craigslist.Org/ Page 1 of 12 pages scraped successfully!\n",
      "Https://Sandiego.Craigslist.Org/ Page 2 of 12 pages scraped successfully!\n",
      "Https://Sandiego.Craigslist.Org/ Page 3 of 12 pages scraped successfully!\n",
      "Https://Sandiego.Craigslist.Org/ Page 4 of 12 pages scraped successfully!\n",
      "Https://Sandiego.Craigslist.Org/ Page 5 of 12 pages scraped successfully!\n",
      "Https://Sandiego.Craigslist.Org/ Page 6 of 12 pages scraped successfully!\n",
      "Https://Sandiego.Craigslist.Org/ Page 7 of 12 pages scraped successfully!\n",
      "Https://Sandiego.Craigslist.Org/ Page 8 of 12 pages scraped successfully!\n",
      "Https://Sandiego.Craigslist.Org/ Page 9 of 12 pages scraped successfully!\n",
      "Https://Sandiego.Craigslist.Org/ Page 10 of 12 pages scraped successfully!\n",
      "Https://Sandiego.Craigslist.Org/ Page 11 of 12 pages scraped successfully!\n",
      "Https://Sandiego.Craigslist.Org/ Page 12 of 12 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Sandiego.Craigslist.Org/ complete!\n",
      "74 rows collected.\n",
      "\n",
      "\n",
      "https://sfbay.craigslist.org/ Posts = 2182\n",
      "Https://Sfbay.Craigslist.Org/ Pages = 19\n",
      "Https://Sfbay.Craigslist.Org/ Page 1 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 2 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 3 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 4 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 5 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 6 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 7 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 8 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 9 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 10 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 11 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 12 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 13 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 14 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 15 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 16 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 17 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 18 of 19 pages scraped successfully!\n",
      "Https://Sfbay.Craigslist.Org/ Page 19 of 19 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Sfbay.Craigslist.Org/ complete!\n",
      "28 rows collected.\n",
      "\n",
      "\n",
      "https://slo.craigslist.org/ Posts = 138\n",
      "Https://Slo.Craigslist.Org/ Pages = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Https://Slo.Craigslist.Org/ Page 1 of 2 pages scraped successfully!\n",
      "Https://Slo.Craigslist.Org/ Page 2 of 2 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Slo.Craigslist.Org/ complete!\n",
      "18 rows collected.\n",
      "\n",
      "\n",
      "https://santabarbara.craigslist.org/ Posts = 67\n",
      "Https://Santabarbara.Craigslist.Org/ Pages = 1\n",
      "Https://Santabarbara.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Santabarbara.Craigslist.Org/ complete!\n",
      "67 rows collected.\n",
      "\n",
      "\n",
      "https://santamaria.craigslist.org/ Posts = 24\n",
      "Https://Santamaria.Craigslist.Org/ Pages = 1\n",
      "Https://Santamaria.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Santamaria.Craigslist.Org/ complete!\n",
      "24 rows collected.\n",
      "\n",
      "\n",
      "https://siskiyou.craigslist.org/ Posts = 5\n",
      "Https://Siskiyou.Craigslist.Org/ Pages = 1\n",
      "Https://Siskiyou.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Siskiyou.Craigslist.Org/ complete!\n",
      "5 rows collected.\n",
      "\n",
      "\n",
      "https://stockton.craigslist.org/ Posts = 87\n",
      "Https://Stockton.Craigslist.Org/ Pages = 1\n",
      "Https://Stockton.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Stockton.Craigslist.Org/ complete!\n",
      "87 rows collected.\n",
      "\n",
      "\n",
      "https://susanville.craigslist.org/ Posts = 4\n",
      "Https://Susanville.Craigslist.Org/ Pages = 1\n",
      "Https://Susanville.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Susanville.Craigslist.Org/ complete!\n",
      "4 rows collected.\n",
      "\n",
      "\n",
      "CPU times: user 16min 43s, sys: 19.8 s, total: 17min 3s\n",
      "Wall time: 8h 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%time title31to50, price31to50, neighborhood31to50, attributes31to50, description31to50, city31to50 = craigslist_motorcycle_scraper(city_list[31:51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL 'hsearch/mca?s=0&bundleDuplicates=1': No schema supplied. Perhaps you meant http://hsearch/mca?s=0&bundleDuplicates=1?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-8f6e6e88d6be>\u001b[0m in \u001b[0;36mcraigslist_motorcycle_scraper\u001b[0;34m(city_list)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#get the first page of the Austin motorcycle prices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}search/mca?s=0&bundleDuplicates=1'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m#parse through it and make it readable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mhtml_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         )\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mprep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_setting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mcookies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged_cookies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         )\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_native_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMissingSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingSchema\u001b[0m: Invalid URL 'hsearch/mca?s=0&bundleDuplicates=1': No schema supplied. Perhaps you meant http://hsearch/mca?s=0&bundleDuplicates=1?"
     ]
    }
   ],
   "source": [
    "%time title15, price15, neighborhood15, attributes15, description15, city15 = craigslist_motorcycle_scraper(city_list[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ventura.craigslist.org/ Posts = 205\n",
      "Https://Ventura.Craigslist.Org/ Pages = 2\n",
      "Https://Ventura.Craigslist.Org/ Page 1 of 2 pages scraped successfully!\n",
      "Https://Ventura.Craigslist.Org/ Page 2 of 2 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Ventura.Craigslist.Org/ complete!\n",
      "85 rows collected.\n",
      "\n",
      "\n",
      "https://visalia.craigslist.org/ Posts = 59\n",
      "Https://Visalia.Craigslist.Org/ Pages = 1\n",
      "Https://Visalia.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Visalia.Craigslist.Org/ complete!\n",
      "59 rows collected.\n",
      "\n",
      "\n",
      "https://yubasutter.craigslist.org/ Posts = 27\n",
      "Https://Yubasutter.Craigslist.Org/ Pages = 1\n",
      "Https://Yubasutter.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Yubasutter.Craigslist.Org/ complete!\n",
      "27 rows collected.\n",
      "\n",
      "\n",
      "https://boulder.craigslist.org/ Posts = 58\n",
      "Https://Boulder.Craigslist.Org/ Pages = 1\n",
      "Https://Boulder.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Boulder.Craigslist.Org/ complete!\n",
      "58 rows collected.\n",
      "\n",
      "\n",
      "https://cosprings.craigslist.org/ Posts = 323\n",
      "Https://Cosprings.Craigslist.Org/ Pages = 3\n",
      "Https://Cosprings.Craigslist.Org/ Page 1 of 3 pages scraped successfully!\n",
      "Https://Cosprings.Craigslist.Org/ Page 2 of 3 pages scraped successfully!\n",
      "Https://Cosprings.Craigslist.Org/ Page 3 of 3 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Cosprings.Craigslist.Org/ complete!\n",
      "83 rows collected.\n",
      "\n",
      "\n",
      "https://denver.craigslist.org/ Posts = 1253\n",
      "Https://Denver.Craigslist.Org/ Pages = 11\n",
      "Https://Denver.Craigslist.Org/ Page 1 of 11 pages scraped successfully!\n",
      "Https://Denver.Craigslist.Org/ Page 2 of 11 pages scraped successfully!\n",
      "Https://Denver.Craigslist.Org/ Page 3 of 11 pages scraped successfully!\n",
      "Https://Denver.Craigslist.Org/ Page 4 of 11 pages scraped successfully!\n",
      "Https://Denver.Craigslist.Org/ Page 5 of 11 pages scraped successfully!\n",
      "Https://Denver.Craigslist.Org/ Page 6 of 11 pages scraped successfully!\n",
      "Https://Denver.Craigslist.Org/ Page 7 of 11 pages scraped successfully!\n",
      "Https://Denver.Craigslist.Org/ Page 8 of 11 pages scraped successfully!\n",
      "Https://Denver.Craigslist.Org/ Page 9 of 11 pages scraped successfully!\n",
      "Https://Denver.Craigslist.Org/ Page 10 of 11 pages scraped successfully!\n",
      "Https://Denver.Craigslist.Org/ Page 11 of 11 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Denver.Craigslist.Org/ complete!\n",
      "53 rows collected.\n",
      "\n",
      "\n",
      "https://eastco.craigslist.org/ Posts = 3\n",
      "Https://Eastco.Craigslist.Org/ Pages = 1\n",
      "Https://Eastco.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Eastco.Craigslist.Org/ complete!\n",
      "3 rows collected.\n",
      "\n",
      "\n",
      "https://fortcollins.craigslist.org/ Posts = 304\n",
      "Https://Fortcollins.Craigslist.Org/ Pages = 3\n",
      "Https://Fortcollins.Craigslist.Org/ Page 1 of 3 pages scraped successfully!\n",
      "Https://Fortcollins.Craigslist.Org/ Page 2 of 3 pages scraped successfully!\n",
      "Https://Fortcollins.Craigslist.Org/ Page 3 of 3 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Fortcollins.Craigslist.Org/ complete!\n",
      "64 rows collected.\n",
      "\n",
      "\n",
      "https://rockies.craigslist.org/ Posts = 37\n",
      "Https://Rockies.Craigslist.Org/ Pages = 1\n",
      "Https://Rockies.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Rockies.Craigslist.Org/ complete!\n",
      "37 rows collected.\n",
      "\n",
      "\n",
      "https://pueblo.craigslist.org/ Posts = 42\n",
      "Https://Pueblo.Craigslist.Org/ Pages = 1\n",
      "Https://Pueblo.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Pueblo.Craigslist.Org/ complete!\n",
      "42 rows collected.\n",
      "\n",
      "\n",
      "https://westslope.craigslist.org/ Posts = 139\n",
      "Https://Westslope.Craigslist.Org/ Pages = 2\n",
      "Https://Westslope.Craigslist.Org/ Page 1 of 2 pages scraped successfully!\n",
      "Https://Westslope.Craigslist.Org/ Page 2 of 2 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Westslope.Craigslist.Org/ complete!\n",
      "19 rows collected.\n",
      "\n",
      "\n",
      "https://newlondon.craigslist.org/ Posts = 233\n",
      "Https://Newlondon.Craigslist.Org/ Pages = 2\n",
      "Https://Newlondon.Craigslist.Org/ Page 1 of 2 pages scraped successfully!\n",
      "Https://Newlondon.Craigslist.Org/ Page 2 of 2 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Newlondon.Craigslist.Org/ complete!\n",
      "113 rows collected.\n",
      "\n",
      "\n",
      "https://hartford.craigslist.org/ Posts = 346\n",
      "Https://Hartford.Craigslist.Org/ Pages = 3\n",
      "Https://Hartford.Craigslist.Org/ Page 1 of 3 pages scraped successfully!\n",
      "Https://Hartford.Craigslist.Org/ Page 2 of 3 pages scraped successfully!\n",
      "Https://Hartford.Craigslist.Org/ Page 3 of 3 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Hartford.Craigslist.Org/ complete!\n",
      "106 rows collected.\n",
      "\n",
      "\n",
      "https://newhaven.craigslist.org/ Posts = 114\n",
      "Https://Newhaven.Craigslist.Org/ Pages = 1\n",
      "Https://Newhaven.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Newhaven.Craigslist.Org/ complete!\n",
      "114 rows collected.\n",
      "\n",
      "\n",
      "https://nwct.craigslist.org/ Posts = 130\n",
      "Https://Nwct.Craigslist.Org/ Pages = 2\n",
      "Https://Nwct.Craigslist.Org/ Page 1 of 2 pages scraped successfully!\n",
      "Https://Nwct.Craigslist.Org/ Page 2 of 2 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Nwct.Craigslist.Org/ complete!\n",
      "10 rows collected.\n",
      "\n",
      "\n",
      "https://delaware.craigslist.org/ Posts = 64\n",
      "Https://Delaware.Craigslist.Org/ Pages = 1\n",
      "Https://Delaware.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Delaware.Craigslist.Org/ complete!\n",
      "64 rows collected.\n",
      "\n",
      "\n",
      "https://washingtondc.craigslist.org/ Posts = 329\n",
      "Https://Washingtondc.Craigslist.Org/ Pages = 3\n",
      "Https://Washingtondc.Craigslist.Org/ Page 1 of 3 pages scraped successfully!\n",
      "Https://Washingtondc.Craigslist.Org/ Page 2 of 3 pages scraped successfully!\n",
      "Https://Washingtondc.Craigslist.Org/ Page 3 of 3 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Washingtondc.Craigslist.Org/ complete!\n",
      "89 rows collected.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8f6e6e88d6be>\u001b[0m in \u001b[0;36mcraigslist_motorcycle_scraper\u001b[0;34m(city_list)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mresults_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'search-legend'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#pulled the total count of posts as the upper bound of the pages array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mresults_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'totalcount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m#each page has 119 posts so each new page is defined as follows: s=120, s=240, s=360, and so on. So we need to step in size 120 in the np.arange function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_total\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "%time title51to70, price51to70, neighborhood51to70, attributes51to70, description51to70, city51to70 = craigslist_motorcycle_scraper(city_list[51:71])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://fortlauderdale.craigslist.org/ Posts = 885\n",
      "Https://Fortlauderdale.Craigslist.Org/ Pages = 8\n",
      "Https://Fortlauderdale.Craigslist.Org/ Page 1 of 8 pages scraped successfully!\n",
      "Https://Fortlauderdale.Craigslist.Org/ Page 2 of 8 pages scraped successfully!\n",
      "Https://Fortlauderdale.Craigslist.Org/ Page 3 of 8 pages scraped successfully!\n",
      "Https://Fortlauderdale.Craigslist.Org/ Page 4 of 8 pages scraped successfully!\n",
      "Https://Fortlauderdale.Craigslist.Org/ Page 5 of 8 pages scraped successfully!\n",
      "Https://Fortlauderdale.Craigslist.Org/ Page 6 of 8 pages scraped successfully!\n",
      "Https://Fortlauderdale.Craigslist.Org/ Page 7 of 8 pages scraped successfully!\n",
      "Https://Fortlauderdale.Craigslist.Org/ Page 8 of 8 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Fortlauderdale.Craigslist.Org/ complete!\n",
      "45 rows collected.\n",
      "\n",
      "\n",
      "https://fortmyers.craigslist.org/ Posts = 243\n",
      "Https://Fortmyers.Craigslist.Org/ Pages = 3\n",
      "Https://Fortmyers.Craigslist.Org/ Page 1 of 3 pages scraped successfully!\n",
      "Https://Fortmyers.Craigslist.Org/ Page 2 of 3 pages scraped successfully!\n",
      "Https://Fortmyers.Craigslist.Org/ Page 3 of 3 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Fortmyers.Craigslist.Org/ complete!\n",
      "3 rows collected.\n",
      "\n",
      "\n",
      "https://gainesville.craigslist.org/ Posts = 104\n",
      "Https://Gainesville.Craigslist.Org/ Pages = 1\n",
      "Https://Gainesville.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Gainesville.Craigslist.Org/ complete!\n",
      "104 rows collected.\n",
      "\n",
      "\n",
      "https://cfl.craigslist.org/ Posts = 12\n",
      "Https://Cfl.Craigslist.Org/ Pages = 1\n",
      "Https://Cfl.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Cfl.Craigslist.Org/ complete!\n",
      "12 rows collected.\n",
      "\n",
      "\n",
      "https://jacksonville.craigslist.org/ Posts = 579\n",
      "Https://Jacksonville.Craigslist.Org/ Pages = 5\n",
      "Https://Jacksonville.Craigslist.Org/ Page 1 of 5 pages scraped successfully!\n",
      "Https://Jacksonville.Craigslist.Org/ Page 2 of 5 pages scraped successfully!\n",
      "Https://Jacksonville.Craigslist.Org/ Page 3 of 5 pages scraped successfully!\n",
      "Https://Jacksonville.Craigslist.Org/ Page 4 of 5 pages scraped successfully!\n",
      "Https://Jacksonville.Craigslist.Org/ Page 5 of 5 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Jacksonville.Craigslist.Org/ complete!\n",
      "89 rows collected.\n",
      "\n",
      "\n",
      "https://lakeland.craigslist.org/ Posts = 29\n",
      "Https://Lakeland.Craigslist.Org/ Pages = 1\n",
      "Https://Lakeland.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Lakeland.Craigslist.Org/ complete!\n",
      "29 rows collected.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8f6e6e88d6be>\u001b[0m in \u001b[0;36mcraigslist_motorcycle_scraper\u001b[0;34m(city_list)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mresults_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'search-legend'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#pulled the total count of posts as the upper bound of the pages array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mresults_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'totalcount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m#each page has 119 posts so each new page is defined as follows: s=120, s=240, s=360, and so on. So we need to step in size 120 in the np.arange function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_total\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "%time title71to100, price71to100, neighborhood71to100, attributes71to100, description71to100, city71to100 = craigslist_motorcycle_scraper(city_list[71:101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://statesboro.craigslist.org/ Posts = 4\n",
      "Https://Statesboro.Craigslist.Org/ Pages = 1\n",
      "Https://Statesboro.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Statesboro.Craigslist.Org/ complete!\n",
      "4 rows collected.\n",
      "\n",
      "\n",
      "https://valdosta.craigslist.org/ Posts = 16\n",
      "Https://Valdosta.Craigslist.Org/ Pages = 1\n",
      "Https://Valdosta.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Valdosta.Craigslist.Org/ complete!\n",
      "16 rows collected.\n",
      "\n",
      "\n",
      "https://honolulu.craigslist.org/ Posts = 482\n",
      "Https://Honolulu.Craigslist.Org/ Pages = 5\n",
      "Https://Honolulu.Craigslist.Org/ Page 1 of 5 pages scraped successfully!\n",
      "Https://Honolulu.Craigslist.Org/ Page 2 of 5 pages scraped successfully!\n",
      "Https://Honolulu.Craigslist.Org/ Page 3 of 5 pages scraped successfully!\n",
      "Https://Honolulu.Craigslist.Org/ Page 4 of 5 pages scraped successfully!\n",
      "Https://Honolulu.Craigslist.Org/ Page 5 of 5 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Honolulu.Craigslist.Org/ complete!\n",
      "2 rows collected.\n",
      "\n",
      "\n",
      "https://boise.craigslist.org/ Posts = 403\n",
      "Https://Boise.Craigslist.Org/ Pages = 4\n",
      "Https://Boise.Craigslist.Org/ Page 1 of 4 pages scraped successfully!\n",
      "Https://Boise.Craigslist.Org/ Page 2 of 4 pages scraped successfully!\n",
      "Https://Boise.Craigslist.Org/ Page 3 of 4 pages scraped successfully!\n",
      "Https://Boise.Craigslist.Org/ Page 4 of 4 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Boise.Craigslist.Org/ complete!\n",
      "43 rows collected.\n",
      "\n",
      "\n",
      "https://eastidaho.craigslist.org/ Posts = 48\n",
      "Https://Eastidaho.Craigslist.Org/ Pages = 1\n",
      "Https://Eastidaho.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Eastidaho.Craigslist.Org/ complete!\n",
      "48 rows collected.\n",
      "\n",
      "\n",
      "https://lewiston.craigslist.org/ Posts = 19\n",
      "Https://Lewiston.Craigslist.Org/ Pages = 1\n",
      "Https://Lewiston.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Lewiston.Craigslist.Org/ complete!\n",
      "19 rows collected.\n",
      "\n",
      "\n",
      "https://twinfalls.craigslist.org/ Posts = 24\n",
      "Https://Twinfalls.Craigslist.Org/ Pages = 1\n",
      "Https://Twinfalls.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Twinfalls.Craigslist.Org/ complete!\n",
      "24 rows collected.\n",
      "\n",
      "\n",
      "https://bn.craigslist.org/ Posts = 19\n",
      "Https://Bn.Craigslist.Org/ Pages = 1\n",
      "Https://Bn.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Bn.Craigslist.Org/ complete!\n",
      "19 rows collected.\n",
      "\n",
      "\n",
      "https://chambana.craigslist.org/ Posts = 37\n",
      "Https://Chambana.Craigslist.Org/ Pages = 1\n",
      "Https://Chambana.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Chambana.Craigslist.Org/ complete!\n",
      "37 rows collected.\n",
      "\n",
      "\n",
      "https://chicago.craigslist.org/ Posts = 1111\n",
      "Https://Chicago.Craigslist.Org/ Pages = 10\n",
      "Https://Chicago.Craigslist.Org/ Page 1 of 10 pages scraped successfully!\n",
      "Https://Chicago.Craigslist.Org/ Page 2 of 10 pages scraped successfully!\n",
      "Https://Chicago.Craigslist.Org/ Page 3 of 10 pages scraped successfully!\n",
      "Https://Chicago.Craigslist.Org/ Page 4 of 10 pages scraped successfully!\n",
      "Https://Chicago.Craigslist.Org/ Page 5 of 10 pages scraped successfully!\n",
      "Https://Chicago.Craigslist.Org/ Page 6 of 10 pages scraped successfully!\n",
      "Https://Chicago.Craigslist.Org/ Page 7 of 10 pages scraped successfully!\n",
      "Https://Chicago.Craigslist.Org/ Page 8 of 10 pages scraped successfully!\n",
      "Https://Chicago.Craigslist.Org/ Page 9 of 10 pages scraped successfully!\n",
      "Https://Chicago.Craigslist.Org/ Page 10 of 10 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Chicago.Craigslist.Org/ complete!\n",
      "31 rows collected.\n",
      "\n",
      "\n",
      "https://decatur.craigslist.org/ Posts = 6\n",
      "Https://Decatur.Craigslist.Org/ Pages = 1\n",
      "Https://Decatur.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Decatur.Craigslist.Org/ complete!\n",
      "6 rows collected.\n",
      "\n",
      "\n",
      "https://lasalle.craigslist.org/ Posts = 11\n",
      "Https://Lasalle.Craigslist.Org/ Pages = 1\n",
      "Https://Lasalle.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Lasalle.Craigslist.Org/ complete!\n",
      "11 rows collected.\n",
      "\n",
      "\n",
      "https://mattoon.craigslist.org/ Posts = 6\n",
      "Https://Mattoon.Craigslist.Org/ Pages = 1\n",
      "Https://Mattoon.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Mattoon.Craigslist.Org/ complete!\n",
      "6 rows collected.\n",
      "\n",
      "\n",
      "https://peoria.craigslist.org/ Posts = 50\n",
      "Https://Peoria.Craigslist.Org/ Pages = 1\n",
      "Https://Peoria.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Peoria.Craigslist.Org/ complete!\n",
      "50 rows collected.\n",
      "\n",
      "\n",
      "https://rockford.craigslist.org/ Posts = 67\n",
      "Https://Rockford.Craigslist.Org/ Pages = 1\n",
      "Https://Rockford.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Rockford.Craigslist.Org/ complete!\n",
      "67 rows collected.\n",
      "\n",
      "\n",
      "https://carbondale.craigslist.org/ Posts = 117\n",
      "Https://Carbondale.Craigslist.Org/ Pages = 1\n",
      "Https://Carbondale.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Carbondale.Craigslist.Org/ complete!\n",
      "117 rows collected.\n",
      "\n",
      "\n",
      "https://springfieldil.craigslist.org/ Posts = 48\n",
      "Https://Springfieldil.Craigslist.Org/ Pages = 1\n"
     ]
    }
   ],
   "source": [
    "%time title101to200, price101to200, neighborhood101to200, attributes101to200, description101to200, city101to200 = craigslist_motorcycle_scraper(city_list[101:201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "for _ in titleto15:\n",
    "    for __ in (_):\n",
    "#         __ = str(_[idx]).split('''>''')[0]\n",
    "#         __ = str(_[idx]).split('''</''')[-2]\n",
    "        title.append(__)\n",
    "    \n",
    "for _ in title16to30:\n",
    "    for __ in (_):\n",
    "#         __ = str(_[idx]).split('''>''')[0]\n",
    "#         __ = str(_[idx]).split('''</''')[-2]\n",
    "        title.append(__)\n",
    "    \n",
    "for _ in title31to50:\n",
    "    for __ in (_):\n",
    "#         __ = str(_[idx]).split('''>''')[0]\n",
    "#         __ = str(_[idx]).split('''</''')[-2]\n",
    "        title.append(__)\n",
    "        \n",
    "price = []\n",
    "\n",
    "for _ in priceto15:\n",
    "    for __ in _:\n",
    "#         __ = str(_[idx]).split('''>''')[1]\n",
    "#         __ = str(_[idx]).split('''</''')[-2]\n",
    "        price.append(__)\n",
    "\n",
    "for _ in price16to30:\n",
    "    for __ in _:\n",
    "#         __ = str(_[idx]).split('''>''')[1]\n",
    "#         __ = str(_[idx]).split('''</''')[-2]\n",
    "        price.append(__)\n",
    "\n",
    "for _ in price31to50:\n",
    "    for __ in _:\n",
    "#         __ = str(_[idx]).split('''>''')[1]\n",
    "#         __ = str(_[idx]).split('''</''')[-2]\n",
    "        price.append(__)\n",
    "        \n",
    "neighborhood = []\n",
    "\n",
    "for _ in neighborhoodto15:\n",
    "    for __ in _:\n",
    "#          __ = str(_[idx]).split('''(''')[1]\n",
    "#         __ = str(_[idx]).split(''')''')[-2]\n",
    "        neighborhood.append(__)\n",
    "    \n",
    "for _ in neighborhood16to30:\n",
    "    for __ in _:\n",
    "#          __ = str(_[idx]).split('''(''')[1]\n",
    "#         __ = str(_[idx]).split(''')''')[-2]\n",
    "        neighborhood.append(__)\n",
    "    \n",
    "for _ in neighborhood31to50:\n",
    "    for __ in _:\n",
    "#          __ = str(_[idx]).split('''(''')[1]\n",
    "#         __ = str(_[idx]).split(''')''')[-2]\n",
    "        neighborhood.append(__)\n",
    "         \n",
    "attributes = []\n",
    "\n",
    "for _ in attributesto15:\n",
    "    for __ in _:\n",
    "        attributes.append(__)\n",
    "        \n",
    "for _ in attributes16to30:\n",
    "    for __ in _:\n",
    "        attributes.append(__)\n",
    "        \n",
    "for _ in attributes31to50:\n",
    "    for __ in _:\n",
    "        attributes.append(__)\n",
    "\n",
    "description = []\n",
    "\n",
    "for _ in descriptionto15:\n",
    "    for __ in _:\n",
    "        description.append(__)\n",
    "        \n",
    "for _ in description16to30:\n",
    "    for __ in _:\n",
    "        description.append(__)\n",
    "        \n",
    "for _ in description31to50:\n",
    "    for __ in _:\n",
    "        description.append(__)\n",
    "        \n",
    "city = []\n",
    "\n",
    "for _ in cityto15:\n",
    "    for __ in _:\n",
    "        city.append(__)\n",
    "        \n",
    "for _ in city16to30:\n",
    "    for __ in _:\n",
    "        city.append(__)\n",
    "        \n",
    "for _ in city31to50:\n",
    "    for __ in _:\n",
    "        city.append(__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11725, 11725, 11725, 11725, 11725, 11725)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(title), len(price), len(neighborhood), len(attributes), len(description), len(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title = np.asarray(title)\n",
    "# price = np.asarray(price)\n",
    "# neighborhood = np.asarray(neighborhood)\n",
    "# attributes = np.asarray(attributes)\n",
    "# description = np.asarray(description)\n",
    "# city = np.asarray(city)\n",
    "\n",
    "data = {'title': title, 'price': price, 'neighborhood': neighborhood, 'attributes': attributes, 'description': description, 'city': city}\n",
    "df = pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>attributes</th>\n",
       "      <th>description</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2016 Honda CBR300r]</td>\n",
       "      <td>[$2900]</td>\n",
       "      <td>[ (Auburn)]</td>\n",
       "      <td>[[\\n, [&lt;b&gt;2016 honda cbr300r&lt;/b&gt;], \\n, [], \\n]...</td>\n",
       "      <td>[\\n, [\\n, [QR Code Link to This Post], \\n, [],...</td>\n",
       "      <td>https://auburn.craigslist.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[BAD CREDIT, NO CREDIT, OK! WE WORK WITH EVERY...</td>\n",
       "      <td>None</td>\n",
       "      <td>[ (FAYETTEVILLE)]</td>\n",
       "      <td>[[\\n, [&lt;b&gt; all bikes&lt;/b&gt;], \\n, [], \\n], [\\n, [...</td>\n",
       "      <td>[\\n, [\\n, [QR Code Link to This Post], \\n, [],...</td>\n",
       "      <td>https://auburn.craigslist.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2017 Yamaha FZ 07 very low miles]</td>\n",
       "      <td>[$5000]</td>\n",
       "      <td>[ (Near Montgomery)]</td>\n",
       "      <td>[[\\n, [&lt;b&gt;2017 Yamaha FZ-07&lt;/b&gt;], \\n, [], \\n],...</td>\n",
       "      <td>[\\n, [\\n, [QR Code Link to This Post], \\n, [],...</td>\n",
       "      <td>https://auburn.craigslist.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2010 Yellow Goldwing GL1800 CSC(see pics/pric...</td>\n",
       "      <td>None</td>\n",
       "      <td>[\\n        (, [google map], )\\n        ]</td>\n",
       "      <td>[[\\n, [&lt;b&gt;2010 Honda GL1800 CSC&lt;/b&gt;], \\n, [], ...</td>\n",
       "      <td>[\\n, [\\n, [QR Code Link to This Post], \\n, [],...</td>\n",
       "      <td>https://auburn.craigslist.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2006 Honda XR650L *Price Drop*]</td>\n",
       "      <td>[$3800]</td>\n",
       "      <td>[ (Auburn)]</td>\n",
       "      <td>[[\\n, [&lt;b&gt;2006 honda xr650l&lt;/b&gt;], \\n, [], \\n],...</td>\n",
       "      <td>[\\n, [\\n, [QR Code Link to This Post], \\n, [],...</td>\n",
       "      <td>https://auburn.craigslist.org/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title    price  \\\n",
       "0                               [2016 Honda CBR300r]  [$2900]   \n",
       "1  [BAD CREDIT, NO CREDIT, OK! WE WORK WITH EVERY...     None   \n",
       "2                 [2017 Yamaha FZ 07 very low miles]  [$5000]   \n",
       "3  [2010 Yellow Goldwing GL1800 CSC(see pics/pric...     None   \n",
       "4                   [2006 Honda XR650L *Price Drop*]  [$3800]   \n",
       "\n",
       "                               neighborhood  \\\n",
       "0                               [ (Auburn)]   \n",
       "1                         [ (FAYETTEVILLE)]   \n",
       "2                      [ (Near Montgomery)]   \n",
       "3  [\\n        (, [google map], )\\n        ]   \n",
       "4                               [ (Auburn)]   \n",
       "\n",
       "                                          attributes  \\\n",
       "0  [[\\n, [<b>2016 honda cbr300r</b>], \\n, [], \\n]...   \n",
       "1  [[\\n, [<b> all bikes</b>], \\n, [], \\n], [\\n, [...   \n",
       "2  [[\\n, [<b>2017 Yamaha FZ-07</b>], \\n, [], \\n],...   \n",
       "3  [[\\n, [<b>2010 Honda GL1800 CSC</b>], \\n, [], ...   \n",
       "4  [[\\n, [<b>2006 honda xr650l</b>], \\n, [], \\n],...   \n",
       "\n",
       "                                         description  \\\n",
       "0  [\\n, [\\n, [QR Code Link to This Post], \\n, [],...   \n",
       "1  [\\n, [\\n, [QR Code Link to This Post], \\n, [],...   \n",
       "2  [\\n, [\\n, [QR Code Link to This Post], \\n, [],...   \n",
       "3  [\\n, [\\n, [QR Code Link to This Post], \\n, [],...   \n",
       "4  [\\n, [\\n, [QR Code Link to This Post], \\n, [],...   \n",
       "\n",
       "                             city  \n",
       "0  https://auburn.craigslist.org/  \n",
       "1  https://auburn.craigslist.org/  \n",
       "2  https://auburn.craigslist.org/  \n",
       "3  https://auburn.craigslist.org/  \n",
       "4  https://auburn.craigslist.org/  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('motorcycle_toy_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<section id=\"postingbody\">\n",
       " <div class=\"print-information print-qrcode-container\">\n",
       " <p class=\"print-qrcode-label\">QR Code Link to This Post</p>\n",
       " <div class=\"print-qrcode\" data-location=\"https://auburn.craigslist.org/mcy/d/auburn-university-2016-honda-cbr300r/7120429265.html\"></div>\n",
       " </div>\n",
       " 2016 Honda CBR300r<br/>\n",
       " 6,186 miles as of listing<br/>\n",
       " after market: fender eliminator kit and bubble windscreen<br/>\n",
       " I have the original parts if you would prefer those.<br/>\n",
       " clean title<br/>\n",
       " <br/>\n",
       " This is an excellent starter bike or daily commuter. 50+ mpg around auburn and the best parking on campus!<br/>\n",
       " <br/>\n",
       " Not looking to trade<br/>\n",
       " <br/>\n",
       " </section>,\n",
       " <section id=\"postingbody\">\n",
       " <div class=\"print-information print-qrcode-container\">\n",
       " <p class=\"print-qrcode-label\">QR Code Link to This Post</p>\n",
       " <div class=\"print-qrcode\" data-location=\"https://auburn.craigslist.org/mcd/d/fayetteville-bad-credit-no-credit-ok-we/7119776172.html\"></div>\n",
       " </div>\n",
       " WE SHIP NATIONWIDE, FINANCE NATIONWIDE! YOU SEE IT! WE SHIP IT! NO PROBLEM!<br/>\n",
       " <br/>\n",
       " FINANCING AVAILABLE. WE CAN GET ALMOST EVERYONE ON A MOTORCYCLE. COME SEE OUR EXPERIENCED AND TRUSTED FINANCE TEAM WHO WILL WORK WITH YOU REGARDLESS OF CREDIT TO MAKE YOU HAPPY.<br/>\n",
       " <br/>\n",
       " FLIP MY CYCLE ONLINE IS A GREAT PLACE TO START TO LOOK.<br/>\n",
       " <br/>\n",
       " Slow pay, bad credit, no credit, divorce, bankruptcy, judgement, past repos and first time buyers.<br/>\n",
       " Have you been turned down before? We can get you riding. We have many lenders that can help you rebuild your credit. We have many finance options available from zero down to minimal down payments. You can apply on line at Flip My Cycle. We are located at 560 N Reilly Road, Fayetteville, NC 28303. WE WELCOME ALL CUSTOMERS REGARDLESS OF THEIR CREDIT SCORE.<br/>\n",
       " <br/>\n",
       " FLIPMYCYCLE.COM HAS OVER 80 MOTORCYCLES TO CHOOSE FROM RANGING IN PRICE FROM $2,000 TO $23,000. WE KEEP OUR BIKES CLEAN AND WITH LOW MILES. A LARGE VARIETY AND GREAT PRICES. WE OFFER BIKES FROM 300cc TO 1800cc. SO, IF YOU RIDE SPORT OR CRUISER WE HAVE THEM ALL. FIRST TIME RIDERS DON'T WORRY WE CAN TEACH YOU AND HELP YOU FIND THE RIGHT BIKE. CHECK US OUT ON THE WEB OR BROWSE OUR PICTURES. <br/>\n",
       " <br/>\n",
       " FLIP MY CYCLE IS YOUR 1 STOP SHOP. WE DO EVERYTHING FROM BUYING YOUR MOTORCYCLE, SELLING YOUR MOTORCYCLE OR REPAIRS. WE HAVE MORE THAN MOST, WITH PRICES BETTER THAN THE REST, WORKING 7 DAYS A WEEK TO BE BETTER THAN THE REST.<br/>\n",
       " <br/>\n",
       " GIVE US A CALL AT 910-779-0737 OR VISIT OUR WEBSITE TO GET PRE-APPROVED NO MATTER YOUR CREDIT. NO CREDIT OR BAD CREDIT IS OUR SPECIALTY.<br/>\n",
       " <br/>\n",
       " WE ARE VETERAN OWNED AND OPERATED LOOKING OUT FOR OUR BROTHERS AND SISTERS IN ARMS.<br/>\n",
       " <br/>\n",
       " SUZUKI GSXR 600, 750, 1000, YAMAHA R1, R6, BMW, DUCATI, TRIUMPH, HARLEY DAVIDSON, STREET GLIDE, ROAD KING, 883, IRON, TRAIN, WIDE GLIDE, KAWASAKI, ZX 10, ZX-14, 636, ZX 6, HONDA CBR 600, CBR 1000, ULTRA GLIDE, 675, 1300, HAYABUSA, 110, 1800, 103, 96 VICTORY, JACKPOT, HIGH BALL, HONDA    </section>,\n",
       " <section id=\"postingbody\">\n",
       " <div class=\"print-information print-qrcode-container\">\n",
       " <p class=\"print-qrcode-label\">QR Code Link to This Post</p>\n",
       " <div class=\"print-qrcode\" data-location=\"https://auburn.craigslist.org/mcy/d/montgomery-2017-yamaha-fz-07-very-low/7118845773.html\"></div>\n",
       " </div>\n",
       " 2017 Yamaha FZ-07 with only 1100 miles, bike is in excellent condition, never ridden hard or crashed.  Selling due to overland build, and need to free up some cash.<br/>\n",
       " <br/>\n",
       " Cash only, deal to be done at my bank during banking hours.  I do not need help selling this bike, I will not buy a vin check (I will provide the vin number to buyer), I will not contact you through your private email which is a known scam.    </section>,\n",
       " <section id=\"postingbody\">\n",
       " <div class=\"print-information print-qrcode-container\">\n",
       " <p class=\"print-qrcode-label\">QR Code Link to This Post</p>\n",
       " <div class=\"print-qrcode\" data-location=\"https://auburn.craigslist.org/mcd/d/haleyville-2010-yellow-goldwing-gl1800/7116263343.html\"></div>\n",
       " </div>\n",
       " 2010 Yellow GL1800 CSC Viper<br/>\n",
       " <br/>\n",
       " LOOK!!! only 8,209 miles<br/>\n",
       " <br/>\n",
       " New (0 miles, 5 year warranty) CSC Viper Kit with EZ Steer (6 degree), Ground Effects, CSC Lighted Spoiler ($990 value), CSC Chrome Lightbar ($675 value), 17 Upgrade Wheels, Upgrade Brakes, Chrome Wheel Well Pkg, Chrome Battery Covers, Lighted Chrome License Plate Bracket, Chrome Grills on Trunk Lights &amp; Tail Lights<br/>\n",
       " <br/>\n",
       " Heated seats, Grips &amp; Toe Warmers, Lighted Spoiler, Kuryakyn Backrest, Fog Lights, Chrome Luggage Rack Chrome Trunk Lid Accent<br/>\n",
       " <br/>\n",
       " $26,900.00<br/>\n",
       " <br/>\n",
       " <br/>\n",
       " <br/>\n",
       " Any questions, call 205-269-0421 or 205-486-9093 or 205-269-0054 or 205-468-1224<br/>\n",
       " see doddcycles.com    </section>,\n",
       " <section id=\"postingbody\">\n",
       " <div class=\"print-information print-qrcode-container\">\n",
       " <p class=\"print-qrcode-label\">QR Code Link to This Post</p>\n",
       " <div class=\"print-qrcode\" data-location=\"https://auburn.craigslist.org/mcy/d/auburn-2006-honda-xr650l-price-drop/7114333691.html\"></div>\n",
       " </div>\n",
       " 2006 Honda XR650L <br/>\n",
       " <br/>\n",
       " 8,000 miles. This number will go up daily<br/>\n",
       " Acerbis long range gas tank (stock tank included w/ wind flares)<br/>\n",
       " ProTaper SE bars<br/>\n",
       " Fmf exhaust + muffler<br/>\n",
       " Aftermarket skid plate <br/>\n",
       " New gripper seat<br/>\n",
       " 50/50 dual-sport tires with 800 mi on bike<br/>\n",
       " New dirt tires included (pictured)<br/>\n",
       " Oil/oil filter changed 200 miles ago<br/>\n",
       " Carb cleaned 100 miles ago<br/>\n",
       " Comes with a backup CDI unit<br/>\n",
       " <br/>\n",
       " This bike starts and rides incredibly. The only negatives are a broken rear blinker and some scratches from trail riding. I ride this bike to work everyday with no problem cruising around 70 mph and ride it on tough single track on the weekends. The XR can really do it all<br/>\n",
       " <br/>\n",
       " Just reduced price to $3800. No trades. Clean title.    </section>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://auburn.craigslist.org/',\n",
       " 'https://auburn.craigslist.org/',\n",
       " 'https://auburn.craigslist.org/',\n",
       " 'https://auburn.craigslist.org/',\n",
       " 'https://auburn.craigslist.org/']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
