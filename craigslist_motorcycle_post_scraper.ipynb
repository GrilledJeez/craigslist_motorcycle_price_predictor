{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import get to call a get request on the site\n",
    "import requests\n",
    "\n",
    "#import to manipulate arrays with numpy\n",
    "import numpy as np\n",
    "\n",
    "#import to create, clean, and parse data frames with pandas\n",
    "import pandas as pd\n",
    "\n",
    "#import to enable datascraping\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#import to set up 'sleep' to wait between page loads\n",
    "import time\n",
    "\n",
    "# import Mongo so our webscraper dumps its scraped data without losing it\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "\n",
    "#import to make that html readable\n",
    "import pprint\n",
    "\n",
    "#import regular expressions operations\n",
    "import re\n",
    "\n",
    "#import to get the universe in balance\n",
    "import random\n",
    "\n",
    "#import so we can do some heavy stats work\n",
    "import scipy as sp\n",
    "from scipy.stats import binom\n",
    "import scipy.stats as stats\n",
    "\n",
    "#import to access certain plotting features\n",
    "import seaborn as sns\n",
    "\n",
    "#import because we need its program functions\n",
    "import math\n",
    "\n",
    "#import because we need to plot and make it pretty\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will collect the links for all cities on craigslist. We will have to sparse through\n",
    "# these and cut out the non-US cities.\n",
    "def city_link_collector():\n",
    "    # this is the craigslist page with every city\n",
    "    main_page = requests.get('https://www.craigslist.org/about/sites')\n",
    "    soup = BeautifulSoup(main_page.text, 'html.parser')\n",
    "    \n",
    "    all_list = []\n",
    "    uscity_list = []\n",
    "    city_list = []\n",
    "    \n",
    "    for i in range(4):\n",
    "        for box in soup.find_all('div', class_='box box_{}'.format(i+1)):\n",
    "            all_list.append(box.find_all('a'))\n",
    "    for _ in all_list[:20:7]:\n",
    "        for __ in _:\n",
    "            uscity_list.append(__)\n",
    "    for ___ in all_list[20][0:94]:\n",
    "        uscity_list.append(___)\n",
    "    \n",
    "    for idx, city in enumerate(uscity_list):\n",
    "        city_list.append(str(uscity_list[idx]).split('''\"''')[1])\n",
    "    \n",
    "    '''These listings are all a subset of Miami and break the scraper if entered in this format'''\n",
    "    city_list.remove('http://miami.craigslist.org/brw/')\n",
    "    city_list.remove('http://miami.craigslist.org/mdc/')\n",
    "    city_list.remove('http://miami.craigslist.org/pbc/')\n",
    "    '''Here is the Miami list to capture the dropped entries and will not break the scraper'''\n",
    "    city_list.append('http://miami.craigslist.org/')\n",
    "    return city_list\n",
    "    #                     #posting date\n",
    "    #                     #grab the datetime element 0 for date and 1 for time\n",
    "    #                     post_datetime = box.find('time', class_= 'result-date')['datetime']\n",
    "    #                     post_timing.append(post_datetime)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_list = city_link_collector()\n",
    "len(city_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def craigslist_motorcycle_scraper(city_list):\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client['craigslist_motorcycles']\n",
    "    post_html = db['motorcycle_posts']\n",
    "    for city in city_list: \n",
    "\n",
    "        #get the first page of the Austin motorcycle prices\n",
    "        city_response = requests.get('{}search/mca?s=0&bundleDuplicates=1'.format(city))\n",
    "        #parse through it and make it readable\n",
    "        html_soup = BeautifulSoup(city_response.text, 'html.parser')\n",
    "        #find the total number of posts to find the limit for each page\n",
    "        results_num = html_soup.find('div', class_= 'search-legend')\n",
    "        #pulled the total count of posts as the upper bound of the pages array\n",
    "        results_total = int(results_num.find('span', class_='totalcount').text) \n",
    "        #each page has 119 posts so each new page is defined as follows: s=120, s=240, s=360, and so on. So we need to step in size 120 in the np.arange function\n",
    "        pages = np.arange(0, results_total+1, 120)\n",
    "        print(\"{} Posts = {}\".format(city, results_total))\n",
    "        print(\"{} Pages = {}\".format(city.title(), len(pages)))\n",
    "\n",
    "        iterations = 0\n",
    "\n",
    "        for page in pages:         \n",
    "            \n",
    "            #get request      \n",
    "            post_response = requests.get(\"{}search/mca?\".format(city) \n",
    "                           + \"s=\" #the parameter for defining the page number \n",
    "                           + str(page) #the page number in the pages array from earlier\n",
    "                           + \"&bundleDuplicates=1\")\n",
    "\n",
    "            time.sleep(random.randint(2,3))\n",
    "\n",
    "            #throw warning for status codes that are not 200\n",
    "            if post_response.status_code != 200:\n",
    "                warn('Request: {}; Status code: {}'.format(requests, post_response.status_code))\n",
    "\n",
    "            #define the html text\n",
    "            post_soup = BeautifulSoup(post_response.text, 'html.parser')\n",
    "\n",
    "            count = 0\n",
    "            for post in post_soup.find_all('a', class_ = 'result-title hdrlnk'):\n",
    "                link = post['href']\n",
    "                count += 1\n",
    "                \n",
    "#               checks to see if post html is loaded in our database and scrapes html if not\n",
    "                if not (post_html.find_one({'_id': link}, {'html': requests.get(link).text})):\n",
    "#                     print(str(link))\n",
    "                    sub_post = requests.get(link)\n",
    "                    post_html.insert_one({'_id': link, 'html': sub_post.text})\n",
    "                    if sub_post.status_code != 200:\n",
    "                        warn('Request: {}; Status code: {}'.format(requests, post_response.status_code))\n",
    "                    time.sleep(random.randint(2,3)) #sleep timer to avoid being banned      \n",
    "                else:\n",
    "#                     print('Passing ' + str(link))\n",
    "                    pass\n",
    "                         \n",
    "                if count == results_total:\n",
    "                    break\n",
    "\n",
    "            iterations += 1\n",
    "            print(\"{} Page \".format(city.title()) + str(iterations) + \" of {} pages\".format(len(pages)) + \" scraped successfully!\")\n",
    "\n",
    "            \n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"{} complete!\".format(city.title()))\n",
    "        print('~' + str(len(pages)*120) + \" rows collected.\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client['craigslist_motorcycles']\n",
    "post_html = db['motorcycle_posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://topeka.craigslist.org/ Posts = 84\n",
      "Https://Topeka.Craigslist.Org/ Pages = 1\n",
      "Https://Topeka.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Topeka.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://wichita.craigslist.org/ Posts = 210\n",
      "Https://Wichita.Craigslist.Org/ Pages = 2\n",
      "Https://Wichita.Craigslist.Org/ Page 1 of 2 pages scraped successfully!\n",
      "Https://Wichita.Craigslist.Org/ Page 2 of 2 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Wichita.Craigslist.Org/ complete!\n",
      "~240 rows collected.\n",
      "\n",
      "\n",
      "https://bgky.craigslist.org/ Posts = 14\n",
      "Https://Bgky.Craigslist.Org/ Pages = 1\n",
      "Https://Bgky.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Bgky.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://eastky.craigslist.org/ Posts = 8\n",
      "Https://Eastky.Craigslist.Org/ Pages = 1\n",
      "Https://Eastky.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Eastky.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://lexington.craigslist.org/ Posts = 81\n",
      "Https://Lexington.Craigslist.Org/ Pages = 1\n",
      "Https://Lexington.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Lexington.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://louisville.craigslist.org/ Posts = 109\n",
      "Https://Louisville.Craigslist.Org/ Pages = 1\n",
      "Https://Louisville.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Louisville.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://owensboro.craigslist.org/ Posts = 7\n",
      "Https://Owensboro.Craigslist.Org/ Pages = 1\n",
      "Https://Owensboro.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Owensboro.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://westky.craigslist.org/ Posts = 45\n",
      "Https://Westky.Craigslist.Org/ Pages = 1\n",
      "Https://Westky.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Westky.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://batonrouge.craigslist.org/ Posts = 19\n",
      "Https://Batonrouge.Craigslist.Org/ Pages = 1\n",
      "Https://Batonrouge.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Batonrouge.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://cenla.craigslist.org/ Posts = 12\n",
      "Https://Cenla.Craigslist.Org/ Pages = 1\n",
      "Https://Cenla.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Cenla.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://houma.craigslist.org/ Posts = 3\n",
      "Https://Houma.Craigslist.Org/ Pages = 1\n",
      "Https://Houma.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Houma.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://lafayette.craigslist.org/ Posts = 11\n",
      "Https://Lafayette.Craigslist.Org/ Pages = 1\n",
      "Https://Lafayette.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Lafayette.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://lakecharles.craigslist.org/ Posts = 6\n",
      "Https://Lakecharles.Craigslist.Org/ Pages = 1\n",
      "Https://Lakecharles.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Lakecharles.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://monroe.craigslist.org/ Posts = 12\n",
      "Https://Monroe.Craigslist.Org/ Pages = 1\n",
      "Https://Monroe.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Monroe.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://neworleans.craigslist.org/ Posts = 57\n",
      "Https://Neworleans.Craigslist.Org/ Pages = 1\n",
      "Https://Neworleans.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Neworleans.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://shreveport.craigslist.org/ Posts = 13\n",
      "Https://Shreveport.Craigslist.Org/ Pages = 1\n",
      "Https://Shreveport.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Shreveport.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://maine.craigslist.org/ Posts = 507\n",
      "Https://Maine.Craigslist.Org/ Pages = 5\n",
      "Https://Maine.Craigslist.Org/ Page 1 of 5 pages scraped successfully!\n",
      "Https://Maine.Craigslist.Org/ Page 2 of 5 pages scraped successfully!\n",
      "Https://Maine.Craigslist.Org/ Page 3 of 5 pages scraped successfully!\n",
      "Https://Maine.Craigslist.Org/ Page 4 of 5 pages scraped successfully!\n",
      "Https://Maine.Craigslist.Org/ Page 5 of 5 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Maine.Craigslist.Org/ complete!\n",
      "~600 rows collected.\n",
      "\n",
      "\n",
      "https://annapolis.craigslist.org/ Posts = 23\n",
      "Https://Annapolis.Craigslist.Org/ Pages = 1\n",
      "Https://Annapolis.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Annapolis.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://baltimore.craigslist.org/ Posts = 128\n",
      "Https://Baltimore.Craigslist.Org/ Pages = 2\n",
      "Https://Baltimore.Craigslist.Org/ Page 1 of 2 pages scraped successfully!\n",
      "Https://Baltimore.Craigslist.Org/ Page 2 of 2 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Baltimore.Craigslist.Org/ complete!\n",
      "~240 rows collected.\n",
      "\n",
      "\n",
      "https://easternshore.craigslist.org/ Posts = 34\n",
      "Https://Easternshore.Craigslist.Org/ Pages = 1\n",
      "Https://Easternshore.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Easternshore.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://frederick.craigslist.org/ Posts = 26\n",
      "Https://Frederick.Craigslist.Org/ Pages = 1\n",
      "Https://Frederick.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Frederick.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://smd.craigslist.org/ Posts = 23\n",
      "Https://Smd.Craigslist.Org/ Pages = 1\n",
      "Https://Smd.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Smd.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://westmd.craigslist.org/ Posts = 16\n",
      "Https://Westmd.Craigslist.Org/ Pages = 1\n",
      "Https://Westmd.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Westmd.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://boston.craigslist.org/ Posts = 930\n",
      "Https://Boston.Craigslist.Org/ Pages = 8\n",
      "Https://Boston.Craigslist.Org/ Page 1 of 8 pages scraped successfully!\n",
      "Https://Boston.Craigslist.Org/ Page 2 of 8 pages scraped successfully!\n",
      "Https://Boston.Craigslist.Org/ Page 3 of 8 pages scraped successfully!\n",
      "Https://Boston.Craigslist.Org/ Page 4 of 8 pages scraped successfully!\n",
      "Https://Boston.Craigslist.Org/ Page 5 of 8 pages scraped successfully!\n",
      "Https://Boston.Craigslist.Org/ Page 6 of 8 pages scraped successfully!\n",
      "Https://Boston.Craigslist.Org/ Page 7 of 8 pages scraped successfully!\n",
      "Https://Boston.Craigslist.Org/ Page 8 of 8 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Boston.Craigslist.Org/ complete!\n",
      "~960 rows collected.\n",
      "\n",
      "\n",
      "https://capecod.craigslist.org/ Posts = 117\n",
      "Https://Capecod.Craigslist.Org/ Pages = 1\n",
      "Https://Capecod.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Capecod.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://southcoast.craigslist.org/ Posts = 87\n",
      "Https://Southcoast.Craigslist.Org/ Pages = 1\n",
      "Https://Southcoast.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Southcoast.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://westernmass.craigslist.org/ Posts = 231\n",
      "Https://Westernmass.Craigslist.Org/ Pages = 2\n",
      "Https://Westernmass.Craigslist.Org/ Page 1 of 2 pages scraped successfully!\n",
      "Https://Westernmass.Craigslist.Org/ Page 2 of 2 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Westernmass.Craigslist.Org/ complete!\n",
      "~240 rows collected.\n",
      "\n",
      "\n",
      "https://worcester.craigslist.org/ Posts = 305\n",
      "Https://Worcester.Craigslist.Org/ Pages = 3\n",
      "Https://Worcester.Craigslist.Org/ Page 1 of 3 pages scraped successfully!\n",
      "Https://Worcester.Craigslist.Org/ Page 2 of 3 pages scraped successfully!\n",
      "Https://Worcester.Craigslist.Org/ Page 3 of 3 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Worcester.Craigslist.Org/ complete!\n",
      "~360 rows collected.\n",
      "\n",
      "\n",
      "https://annarbor.craigslist.org/ Posts = 43\n",
      "Https://Annarbor.Craigslist.Org/ Pages = 1\n",
      "Https://Annarbor.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Annarbor.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://battlecreek.craigslist.org/ Posts = 9\n",
      "Https://Battlecreek.Craigslist.Org/ Pages = 1\n",
      "Https://Battlecreek.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Battlecreek.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://centralmich.craigslist.org/ Posts = 27\n",
      "Https://Centralmich.Craigslist.Org/ Pages = 1\n",
      "Https://Centralmich.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Centralmich.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://detroit.craigslist.org/ Posts = 565\n",
      "Https://Detroit.Craigslist.Org/ Pages = 5\n",
      "Https://Detroit.Craigslist.Org/ Page 1 of 5 pages scraped successfully!\n",
      "Https://Detroit.Craigslist.Org/ Page 2 of 5 pages scraped successfully!\n",
      "Https://Detroit.Craigslist.Org/ Page 3 of 5 pages scraped successfully!\n",
      "Https://Detroit.Craigslist.Org/ Page 4 of 5 pages scraped successfully!\n",
      "Https://Detroit.Craigslist.Org/ Page 5 of 5 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Detroit.Craigslist.Org/ complete!\n",
      "~600 rows collected.\n",
      "\n",
      "\n",
      "https://flint.craigslist.org/ Posts = 35\n",
      "Https://Flint.Craigslist.Org/ Pages = 1\n",
      "Https://Flint.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Flint.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://grandrapids.craigslist.org/ Posts = 174\n",
      "Https://Grandrapids.Craigslist.Org/ Pages = 2\n",
      "Https://Grandrapids.Craigslist.Org/ Page 1 of 2 pages scraped successfully!\n",
      "Https://Grandrapids.Craigslist.Org/ Page 2 of 2 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Grandrapids.Craigslist.Org/ complete!\n",
      "~240 rows collected.\n",
      "\n",
      "\n",
      "https://holland.craigslist.org/ Posts = 11\n",
      "Https://Holland.Craigslist.Org/ Pages = 1\n",
      "Https://Holland.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Holland.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://jxn.craigslist.org/ Posts = 18\n",
      "Https://Jxn.Craigslist.Org/ Pages = 1\n",
      "Https://Jxn.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Jxn.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://kalamazoo.craigslist.org/ Posts = 46\n",
      "Https://Kalamazoo.Craigslist.Org/ Pages = 1\n",
      "Https://Kalamazoo.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Kalamazoo.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://lansing.craigslist.org/ Posts = 50\n",
      "Https://Lansing.Craigslist.Org/ Pages = 1\n",
      "Https://Lansing.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Lansing.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://monroemi.craigslist.org/ Posts = 8\n",
      "Https://Monroemi.Craigslist.Org/ Pages = 1\n",
      "Https://Monroemi.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Monroemi.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://muskegon.craigslist.org/ Posts = 24\n",
      "Https://Muskegon.Craigslist.Org/ Pages = 1\n",
      "Https://Muskegon.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Muskegon.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://nmi.craigslist.org/ Posts = 105\n",
      "Https://Nmi.Craigslist.Org/ Pages = 1\n",
      "Https://Nmi.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Nmi.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://porthuron.craigslist.org/ Posts = 22\n",
      "Https://Porthuron.Craigslist.Org/ Pages = 1\n",
      "Https://Porthuron.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Porthuron.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://saginaw.craigslist.org/ Posts = 37\n",
      "Https://Saginaw.Craigslist.Org/ Pages = 1\n",
      "Https://Saginaw.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Saginaw.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://swmi.craigslist.org/ Posts = 18\n",
      "Https://Swmi.Craigslist.Org/ Pages = 1\n",
      "Https://Swmi.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Swmi.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://thumb.craigslist.org/ Posts = 6\n",
      "Https://Thumb.Craigslist.Org/ Pages = 1\n",
      "Https://Thumb.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Thumb.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://up.craigslist.org/ Posts = 39\n",
      "Https://Up.Craigslist.Org/ Pages = 1\n",
      "Https://Up.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Up.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://bemidji.craigslist.org/ Posts = 23\n",
      "Https://Bemidji.Craigslist.Org/ Pages = 1\n",
      "Https://Bemidji.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Bemidji.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://brainerd.craigslist.org/ Posts = 31\n",
      "Https://Brainerd.Craigslist.Org/ Pages = 1\n",
      "Https://Brainerd.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Brainerd.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://duluth.craigslist.org/ Posts = 147\n",
      "Https://Duluth.Craigslist.Org/ Pages = 2\n",
      "Https://Duluth.Craigslist.Org/ Page 1 of 2 pages scraped successfully!\n",
      "Https://Duluth.Craigslist.Org/ Page 2 of 2 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Duluth.Craigslist.Org/ complete!\n",
      "~240 rows collected.\n",
      "\n",
      "\n",
      "https://mankato.craigslist.org/ Posts = 69\n",
      "Https://Mankato.Craigslist.Org/ Pages = 1\n",
      "Https://Mankato.Craigslist.Org/ Page 1 of 1 pages scraped successfully!\n",
      "\n",
      "\n",
      "Https://Mankato.Craigslist.Org/ complete!\n",
      "~120 rows collected.\n",
      "\n",
      "\n",
      "https://minneapolis.craigslist.org/ Posts = 1320\n",
      "Https://Minneapolis.Craigslist.Org/ Pages = 12\n",
      "Https://Minneapolis.Craigslist.Org/ Page 1 of 12 pages scraped successfully!\n",
      "Https://Minneapolis.Craigslist.Org/ Page 2 of 12 pages scraped successfully!\n",
      "Https://Minneapolis.Craigslist.Org/ Page 3 of 12 pages scraped successfully!\n",
      "Https://Minneapolis.Craigslist.Org/ Page 4 of 12 pages scraped successfully!\n",
      "Https://Minneapolis.Craigslist.Org/ Page 5 of 12 pages scraped successfully!\n",
      "Https://Minneapolis.Craigslist.Org/ Page 6 of 12 pages scraped successfully!\n",
      "Https://Minneapolis.Craigslist.Org/ Page 7 of 12 pages scraped successfully!\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='minneapolis.craigslist.org', port=443): Max retries exceeded with url: /ram/mcd/d/forest-lake-2020-lance-powersports-cabo/7109535354.html (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fc35bc94b50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             )\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.VerifiedHTTPSConnection object at 0x7fc35bc94b50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    719\u001b[0m             retries = retries.increment(\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='minneapolis.craigslist.org', port=443): Max retries exceeded with url: /ram/mcd/d/forest-lake-2020-lance-powersports-cabo/7109535354.html (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fc35bc94b50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e5ca355258c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcraigslist_motorcycle_scraper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m143\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-ce3b831cb06c>\u001b[0m in \u001b[0;36mcraigslist_motorcycle_scraper\u001b[0;34m(city_list)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#               checks to see if post html is loaded in our database and scrapes html if not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpost_html\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'html'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;31m#                     print(str(link))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0msub_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='minneapolis.craigslist.org', port=443): Max retries exceeded with url: /ram/mcd/d/forest-lake-2020-lance-powersports-cabo/7109535354.html (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fc35bc94b50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))"
     ]
    }
   ],
   "source": [
    "craigslist_motorcycle_scraper(city_list[143:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://minneapolis.craigslist.org/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_list[193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
