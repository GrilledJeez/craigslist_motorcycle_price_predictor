{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import get to call a get request on the site\n",
    "import requests\n",
    "\n",
    "#import to manipulate arrays with numpy\n",
    "import numpy as np\n",
    "\n",
    "#import to create, clean, and parse data frames with pandas\n",
    "import pandas as pd\n",
    "\n",
    "#import to enable datascraping\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#import to set up 'sleep' to wait between page loads\n",
    "import time\n",
    "\n",
    "# import Mongo so our webscraper dumps its scraped data without losing it\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "\n",
    "#import to make that html readable\n",
    "import pprint\n",
    "\n",
    "#import regular expressions operations\n",
    "import re\n",
    "\n",
    "#import to get the universe in balance\n",
    "import random\n",
    "\n",
    "#import so we can do some heavy stats work\n",
    "import scipy as sp\n",
    "from scipy.stats import binom\n",
    "import scipy.stats as stats\n",
    "\n",
    "#import to access certain plotting features\n",
    "import seaborn as sns\n",
    "\n",
    "#import because we need its program functions\n",
    "import math\n",
    "\n",
    "#import because we need to plot and make it pretty\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def craigslist_motorcycle_scraper(city_list):\n",
    "\n",
    "    title = []\n",
    "    price = []\n",
    "    neighborhood = []\n",
    "    attributes = []\n",
    "    description = []\n",
    "    city_names = []\n",
    "\n",
    "    for city in city_list: \n",
    "\n",
    "        #get the first page of the Austin motorcycle prices\n",
    "        response = requests.get('{}search/mca?s=0&bundleDuplicates=1'.format(city))\n",
    "        #parse through it and make it readable\n",
    "        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        #find the total number of posts to find the limit for each page\n",
    "        results_num = html_soup.find('div', class_= 'search-legend')\n",
    "        #pulled the total count of posts as the upper bound of the pages array\n",
    "        results_total = int(results_num.find('span', class_='totalcount').text) \n",
    "        #each page has 119 posts so each new page is defined as follows: s=120, s=240, s=360, and so on. So we need to step in size 120 in the np.arange function\n",
    "        pages = np.arange(0, results_total+1, 120)\n",
    "        print(\"{} Posts = {}\".format(city, results_total))\n",
    "        print(\"{} Pages = {}\".format(city.title(), len(pages)))\n",
    "\n",
    "        iterations = 0\n",
    "\n",
    "        for page in pages:         \n",
    "            title_ = []\n",
    "            price_ = []\n",
    "            neighborhood_ = []\n",
    "            attributes_ = []\n",
    "            description_ = []\n",
    "            city_names_ = []\n",
    "            #get request      \n",
    "            response = requests.get(\"{}search/mca?\".format(city) \n",
    "                           + \"s=\" #the parameter for defining the page number \n",
    "                           + str(page) #the page number in the pages array from earlier\n",
    "                           + \"&bundleDuplicates=1\")\n",
    "\n",
    "            time.sleep(random.randint(2,3))\n",
    "\n",
    "            #throw warning for status codes that are not 200\n",
    "            if response.status_code != 200:\n",
    "                warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "            #define the html text\n",
    "            post_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            #define the posts\n",
    "    #         posts = page_html.find_all('li', class_= 'result-row')\n",
    "\n",
    "            #extract data by item\n",
    "    #         for post in posts:\n",
    "            count = 0\n",
    "            for post in post_soup.find_all('a', class_ = 'result-title hdrlnk'):\n",
    "                link = post['href']\n",
    "                sub_post = requests.get(link)\n",
    "                sub_soup = BeautifulSoup(sub_post.text, 'html.parser')\n",
    "\n",
    "    #             if tag.find('small') is not None:\n",
    "\n",
    "                post_title = sub_soup.find('span', id = 'titletextonly')\n",
    "                title_.append(post_title)\n",
    "\n",
    "                post_price = sub_soup.find('span', class_ = 'price')\n",
    "                price_.append(post_price)\n",
    "\n",
    "                post_neighborhood = sub_soup.find('small')\n",
    "                neighborhood_.append(post_neighborhood)\n",
    "\n",
    "                post_attributes = sub_soup.find_all('p', attrs = {'class': 'attrgroup'})\n",
    "                attributes_.append(post_attributes)\n",
    "\n",
    "                post_description = sub_soup.find('section', id = 'postingbody')\n",
    "                description_.append(post_description)\n",
    "\n",
    "                city_names_.append(city)\n",
    "\n",
    "                time.sleep(random.randint(2,3))\n",
    "                count += 1\n",
    "                \n",
    "                if count == results_total:\n",
    "                    break\n",
    "\n",
    "            iterations += 1\n",
    "            print(\"{} Page \".format(city.title()) + str(iterations) + \" of {} pages\".format(len(pages)) + \" scraped successfully!\")\n",
    "\n",
    "            title.append(title_)\n",
    "            price.append(price_)\n",
    "            neighborhood.append(neighborhood_)\n",
    "            attributes.append(attributes_)\n",
    "            description.append(description_)\n",
    "            city_names.append(city_names_)\n",
    "\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"{} complete!\".format(city.title()))\n",
    "        print(str(len(title_)) + \" rows collected.\")\n",
    "        print(\"\\n\")\n",
    "    return title, price, neighborhood, attributes, description, city_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "\n",
    "for _ in title15:\n",
    "    for __ in (_):\n",
    "#         __ = str(_[idx]).split('''>''')[0]\n",
    "#         __ = str(_[idx]).split('''</''')[-2]\n",
    "        title.append(__)\n",
    "\n",
    "price = []\n",
    "\n",
    "for _ in price15:\n",
    "    for __ in _:\n",
    "#         __ = str(_[idx]).split('''>''')[1]\n",
    "#         __ = str(_[idx]).split('''</''')[-2]\n",
    "        price.append(__)\n",
    "\n",
    "neighborhood = []\n",
    "\n",
    "for _ in neighborhood15:\n",
    "    for __ in _:\n",
    "#          __ = str(_[idx]).split('''(''')[1]\n",
    "#         __ = str(_[idx]).split(''')''')[-2]\n",
    "        neighborhood.append(__)\n",
    "\n",
    "attributes = []\n",
    "\n",
    "for _ in attributes15:\n",
    "    for __ in _:\n",
    "        attributes.append(__)\n",
    "\n",
    "description = []\n",
    "\n",
    "for _ in description15:\n",
    "    for __ in _:\n",
    "        description.append(__)\n",
    "\n",
    "city = []\n",
    "\n",
    "for _ in city15:\n",
    "    for __ in _:\n",
    "        city.append(__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
